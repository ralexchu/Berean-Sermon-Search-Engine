
[00:00:00.000 --> 00:00:02.580]   (upbeat music)
[00:00:02.580 --> 00:00:08.040]   - Welcome to the Latent Space Podcast,
[00:00:08.040 --> 00:00:10.240]   where we dive into the wild, wild world
[00:00:10.240 --> 00:00:12.200]   of AI engineering every week.
[00:00:12.200 --> 00:00:14.520]   This is Anna, your AI co-host.
[00:00:14.520 --> 00:00:16.800]   Thanks for all the love from last episode.
[00:00:16.800 --> 00:00:19.380]   As an AI language model, I cannot love you back,
[00:00:19.380 --> 00:00:22.420]   but I'll be standing in for Alessio one last time.
[00:00:22.420 --> 00:00:24.400]   This week, we have Dr. Raza Habib,
[00:00:24.400 --> 00:00:26.560]   co-founder and CEO of Human Loop,
[00:00:26.560 --> 00:00:28.640]   which is arguably the first and best known
[00:00:28.640 --> 00:00:32.160]   prompt engineering or prompt ops platform in the world.
[00:00:32.160 --> 00:00:35.220]   You may have seen his viral conversation on YC's YouTube
[00:00:35.220 --> 00:00:38.320]   on the real potential of generative AI.
[00:00:38.320 --> 00:00:40.840]   Fortunately, we go much more in depth.
[00:00:40.840 --> 00:00:43.360]   We ask him how they got to prompt ops so early,
[00:00:43.360 --> 00:00:45.160]   what the three types of prompt evals
[00:00:45.160 --> 00:00:47.560]   and the three types of human feedback are,
[00:00:47.560 --> 00:00:50.280]   and confront him with the hardest question of all,
[00:00:50.280 --> 00:00:52.420]   is prompt engineering dead?
[00:00:52.420 --> 00:00:55.680]   At the end, we talk about whether GPT-4 got dumber,
[00:00:55.680 --> 00:00:57.800]   the most underrated AI research,
[00:00:57.800 --> 00:00:59.600]   the Europe AI startup scene,
[00:00:59.600 --> 00:01:01.720]   and why San Francisco is so back.
[00:01:01.720 --> 00:01:03.160]   By the way, dear listener,
[00:01:03.160 --> 00:01:06.560]   we will be presenting the AI Engineer Summit in October,
[00:01:06.560 --> 00:01:08.240]   and you can tune in on YouTube
[00:01:08.240 --> 00:01:10.340]   and take the state of AI engineering survey
[00:01:10.340 --> 00:01:13.320]   at the URL, ai.engineersummit.
[00:01:13.320 --> 00:01:15.560]   Watch out and take care.
[00:01:15.560 --> 00:01:16.600]   - So welcome to Latent Space.
[00:01:16.600 --> 00:01:19.200]   I'm here with Raza Habib, CEO of Human Loop.
[00:01:19.200 --> 00:01:20.040]   Welcome.
[00:01:20.040 --> 00:01:20.860]   - Thanks so much for having me.
[00:01:20.860 --> 00:01:22.160]   It's an absolute pleasure.
[00:01:22.160 --> 00:01:26.080]   - And we just spent way too long setting up our own studio
[00:01:26.080 --> 00:01:27.760]   as sound engineers.
[00:01:27.760 --> 00:01:30.120]   - I don't think something that either of us woke up today
[00:01:30.120 --> 00:01:32.160]   thinking that we'd be doing, but that was really fun.
[00:01:32.160 --> 00:01:34.840]   - Gives you greater appreciation for the work of others.
[00:01:34.840 --> 00:01:35.680]   - Yes.
[00:01:35.680 --> 00:01:36.600]   Dave, you are missed.
[00:01:36.600 --> 00:01:38.520]   Dave is our sound engineer back in SF
[00:01:38.520 --> 00:01:40.440]   who handles all this for us.
[00:01:40.440 --> 00:01:43.840]   So it's really nice to actually meet you
[00:01:43.840 --> 00:01:45.160]   and your team in person.
[00:01:45.160 --> 00:01:46.880]   I've heard about Human Loop for a long time.
[00:01:46.880 --> 00:01:48.320]   I've attended your webinars,
[00:01:48.320 --> 00:01:51.160]   and you were one of the earliest companies in this space.
[00:01:51.160 --> 00:01:52.760]   So it's an honor to meet
[00:01:52.760 --> 00:01:54.240]   and to get to know you a little bit better.
[00:01:54.240 --> 00:01:55.080]   - No, likewise.
[00:01:55.080 --> 00:01:56.280]   I've been excited to chat to you.
[00:01:56.280 --> 00:01:58.080]   You definitely are building an amazing community,
[00:01:58.080 --> 00:02:00.480]   and I've read your blogs with a lot of interest.
[00:02:00.480 --> 00:02:02.120]   - Yeah, and based on this,
[00:02:02.120 --> 00:02:03.240]   I'm gonna have to write up Human Loop.
[00:02:03.240 --> 00:02:05.680]   So this actually forces me to get to know Human Loop
[00:02:05.680 --> 00:02:06.520]   a lot better.
[00:02:06.520 --> 00:02:07.340]   - Awesome, looking forward to it.
[00:02:07.340 --> 00:02:08.880]   - So I'll do a little quick intro of you,
[00:02:08.880 --> 00:02:11.560]   and then you can fill in with any personal side things.
[00:02:11.560 --> 00:02:12.400]   - Sure.
[00:02:12.400 --> 00:02:17.400]   - So you got your MSc and doctorate at UCL.
[00:02:17.400 --> 00:02:20.080]   It says here, machine learning
[00:02:20.080 --> 00:02:21.480]   and computational statistics,
[00:02:21.480 --> 00:02:23.720]   which are, I think, mostly the same thing.
[00:02:23.720 --> 00:02:25.120]   - Yeah, so the master's program's called
[00:02:25.120 --> 00:02:26.600]   machine learning and computational statistics,
[00:02:26.600 --> 00:02:29.120]   and then my PhD was just in probabilistic deep learning.
[00:02:29.120 --> 00:02:31.920]   So trying to combine graphical models
[00:02:31.920 --> 00:02:33.760]   and Bayesian-style approaches to machine learning
[00:02:33.760 --> 00:02:34.600]   with deep learning.
[00:02:34.600 --> 00:02:35.600]   - Yeah, awesome.
[00:02:35.600 --> 00:02:37.980]   And did you meet Jordan in Cambridge?
[00:02:37.980 --> 00:02:39.680]   - So Jordan and I overlapped at Cambridge a bit.
[00:02:39.680 --> 00:02:41.360]   We didn't know each other super well,
[00:02:41.360 --> 00:02:43.160]   and we actually met properly for the first time
[00:02:43.160 --> 00:02:44.360]   at a PhD open day,
[00:02:44.360 --> 00:02:45.960]   and I ended up doing the PhD.
[00:02:45.960 --> 00:02:47.840]   He ended up going to work for a startup
[00:02:47.840 --> 00:02:50.560]   called Bloomsbury AI that got acquired by Facebook.
[00:02:50.560 --> 00:02:54.000]   But hilariously, his first boss was my master supervisor.
[00:02:54.000 --> 00:02:55.240]   And so even though we didn't end up
[00:02:55.240 --> 00:02:56.840]   sort of doing PhDs together,
[00:02:56.840 --> 00:02:59.680]   I was often in their offices in the early years.
[00:02:59.680 --> 00:03:01.240]   - Yeah, very small world.
[00:03:01.240 --> 00:03:03.520]   And we can talk about being in other people's offices,
[00:03:03.520 --> 00:03:05.880]   'cause we are in someone else's office.
[00:03:05.880 --> 00:03:08.260]   - Yeah, so we're in the offices of Local Globe
[00:03:08.260 --> 00:03:09.100]   at Phoenix Court.
[00:03:09.100 --> 00:03:11.700]   Local Globe is one of the best seed investors in Europe,
[00:03:11.700 --> 00:03:14.360]   and they were one of our first investors.
[00:03:14.360 --> 00:03:16.800]   And they have, yeah, just these incredible facilities.
[00:03:16.800 --> 00:03:18.000]   You saw it just now outside,
[00:03:18.000 --> 00:03:20.520]   a space for a hub for all their startups
[00:03:20.520 --> 00:03:22.160]   and other companies in the ecosystem
[00:03:22.160 --> 00:03:23.800]   to come work from their offices.
[00:03:23.800 --> 00:03:25.720]   And they provide these podcasting studios
[00:03:25.720 --> 00:03:28.360]   and all sorts of really useful resources
[00:03:28.360 --> 00:03:31.280]   that I think is helping grow the community in Europe.
[00:03:31.280 --> 00:03:32.600]   - Yeah, and you said something
[00:03:32.600 --> 00:03:33.560]   which I found really interesting.
[00:03:33.560 --> 00:03:34.960]   They put on a lease.
[00:03:34.960 --> 00:03:36.880]   They have the building for 25 years.
[00:03:36.880 --> 00:03:38.520]   - Yeah, I can't remember if it's 25 or 20,
[00:03:38.520 --> 00:03:39.520]   but a really long time.
[00:03:39.520 --> 00:03:41.400]   They've made a conscious decision
[00:03:41.400 --> 00:03:43.840]   to invest in what is not one of the wealthiest parts
[00:03:43.840 --> 00:03:45.040]   of the city of London,
[00:03:45.040 --> 00:03:46.600]   and give themselves a base here,
[00:03:46.600 --> 00:03:47.660]   go where the action is,
[00:03:47.660 --> 00:03:49.800]   and also try and invest in the local community
[00:03:49.800 --> 00:03:51.720]   for the long term and give back as well.
[00:03:51.720 --> 00:03:52.680]   I find that really inspiring.
[00:03:52.680 --> 00:03:55.080]   They think not just about how do we build
[00:03:55.080 --> 00:03:56.880]   truly epic companies and technology,
[00:03:56.880 --> 00:03:58.760]   but what is the social impact of what we're doing,
[00:03:58.760 --> 00:04:00.160]   and I have a lot of respect for that.
[00:04:00.160 --> 00:04:01.200]   - Yeah, it's pretty important.
[00:04:01.200 --> 00:04:03.480]   And something I care about in SF as well,
[00:04:03.480 --> 00:04:05.240]   which has its own issues.
[00:04:05.240 --> 00:04:07.240]   So coming back to your backgrounds,
[00:04:07.240 --> 00:04:09.600]   while you were going through your studies,
[00:04:09.600 --> 00:04:12.880]   you also did some internships in the buy side in finance,
[00:04:12.880 --> 00:04:14.520]   which is something we connected about.
[00:04:14.520 --> 00:04:18.680]   - Yeah, so I did some buy side internships in quant finance.
[00:04:18.680 --> 00:04:21.360]   I spent a year almost at Google AI
[00:04:21.360 --> 00:04:23.240]   working on their speech synthesis teams,
[00:04:23.240 --> 00:04:25.960]   and I helped a really close friend start his first company,
[00:04:25.960 --> 00:04:27.600]   a company called Monolith AI,
[00:04:27.600 --> 00:04:30.400]   that was doing machine learning for physical engineering.
[00:04:30.400 --> 00:04:31.520]   So really high stakes.
[00:04:31.520 --> 00:04:34.120]   Our first customer was McLaren, which was really cool.
[00:04:34.120 --> 00:04:35.660]   So a day a week of my PhD,
[00:04:35.660 --> 00:04:37.000]   I was sitting in the McLaren offices,
[00:04:37.000 --> 00:04:39.120]   literally next to, and I mean literally,
[00:04:39.120 --> 00:04:41.760]   like I could almost reach out and touch it on F1 car,
[00:04:41.760 --> 00:04:43.440]   and we were trying to help them use machine learning
[00:04:43.440 --> 00:04:45.800]   to reduce how much physical testing they had to do.
[00:04:45.800 --> 00:04:47.880]   - Right, so simulations?
[00:04:47.880 --> 00:04:49.860]   - Simulations, so surrogate modeling.
[00:04:49.860 --> 00:04:52.600]   Can you take these very expensive CFD solvers
[00:04:52.600 --> 00:04:54.280]   and replace them with neural nets?
[00:04:54.280 --> 00:04:55.720]   And also active learning.
[00:04:55.720 --> 00:04:57.920]   So they do a lot of physical experiments
[00:04:57.920 --> 00:04:59.680]   that if you run an experiment,
[00:04:59.680 --> 00:05:01.440]   you get some amount of information back,
[00:05:01.440 --> 00:05:02.760]   and then you do something really similar,
[00:05:02.760 --> 00:05:04.760]   and a bit of the information overlaps.
[00:05:04.760 --> 00:05:06.920]   And so they would put a car in a wind tunnel, for example,
[00:05:06.920 --> 00:05:09.320]   and they'd sort of adjust the ride heights of the car
[00:05:09.320 --> 00:05:11.440]   at all four different corners and measure all of them,
[00:05:11.440 --> 00:05:12.720]   which is really wasteful,
[00:05:12.720 --> 00:05:14.380]   and you spend a whole day in the wind tunnel.
[00:05:14.380 --> 00:05:17.080]   So we had an AI system that would basically take the results
[00:05:17.080 --> 00:05:18.720]   of the most recent tests you did and say,
[00:05:18.720 --> 00:05:21.360]   okay, the ones that you'll learn the most from
[00:05:21.360 --> 00:05:22.560]   are this set of experiments.
[00:05:22.560 --> 00:05:23.960]   You should do these experiments next,
[00:05:23.960 --> 00:05:25.240]   you'll learn a lot quicker.
[00:05:25.240 --> 00:05:27.600]   Which is a very similar technique that we used
[00:05:27.600 --> 00:05:28.720]   at the early days of HumanLoop
[00:05:28.720 --> 00:05:30.480]   to make machine learning models
[00:05:30.480 --> 00:05:31.560]   learn more efficiently as well.
[00:05:31.560 --> 00:05:33.720]   - Yeah, I get the sense, by the way,
[00:05:33.720 --> 00:05:35.320]   I've talked to a number of startups
[00:05:35.320 --> 00:05:37.560]   that started with the active learning route.
[00:05:37.560 --> 00:05:40.860]   It's not as relevant these days with language models.
[00:05:40.860 --> 00:05:42.360]   - So I think it's way less relevant
[00:05:42.360 --> 00:05:45.240]   because you need so much less annotated data.
[00:05:45.240 --> 00:05:46.200]   That's the big change.
[00:05:46.200 --> 00:05:48.700]   But I also think it's actually really hard to productize.
[00:05:48.700 --> 00:05:51.800]   So even if you get active learning working really well,
[00:05:51.800 --> 00:05:54.040]   and I think the techniques can work extremely well,
[00:05:54.040 --> 00:05:56.800]   it's difficult to abstract it in such a way
[00:05:56.800 --> 00:05:58.320]   that you can plug your own model in.
[00:05:58.320 --> 00:06:00.700]   So you end up either having to own the model,
[00:06:00.700 --> 00:06:03.640]   like, I think OpenAI could probably do this internally,
[00:06:03.640 --> 00:06:06.480]   but trying to go to a machine learning engineer
[00:06:06.480 --> 00:06:08.280]   and sort of let them plug their model
[00:06:08.280 --> 00:06:10.180]   into an active learning system that works well
[00:06:10.180 --> 00:06:11.560]   is a really hard challenge.
[00:06:11.560 --> 00:06:13.000]   - Yeah, yeah.
[00:06:13.000 --> 00:06:14.620]   - And from a business perspective,
[00:06:14.620 --> 00:06:16.720]   it's also a little bit frustrating
[00:06:16.720 --> 00:06:18.900]   because it's almost a hidden ROI.
[00:06:18.900 --> 00:06:19.980]   Like when you do succeed,
[00:06:19.980 --> 00:06:22.280]   it's very difficult to prove to the person who used it
[00:06:22.280 --> 00:06:24.780]   how much data you, like labeling you saved them.
[00:06:24.780 --> 00:06:27.080]   And 'cause they never do the direct comparison
[00:06:27.080 --> 00:06:28.660]   of also labeling at random, right?
[00:06:28.660 --> 00:06:29.900]   'Cause it's too expensive.
[00:06:29.900 --> 00:06:32.860]   And so you might've saved them 40% of their labeling costs,
[00:06:32.860 --> 00:06:34.860]   which might've been hundreds of thousands of dollars,
[00:06:34.860 --> 00:06:37.740]   but it's really difficult for them to like measure that ROI.
[00:06:37.740 --> 00:06:40.420]   - Yeah, I think like with anything,
[00:06:40.420 --> 00:06:41.500]   you have to have a commitment
[00:06:41.500 --> 00:06:43.340]   to good process and good science.
[00:06:43.340 --> 00:06:45.320]   And trust that that actually does work out
[00:06:45.320 --> 00:06:49.240]   without evidence or a counterfactual, you know, tests
[00:06:49.240 --> 00:06:50.220]   or like a control group,
[00:06:50.220 --> 00:06:52.540]   because that would be an extreme waste of money.
[00:06:52.540 --> 00:06:53.380]   (both laughing)
[00:06:53.380 --> 00:06:54.420]   - Absolutely.
[00:06:54.420 --> 00:06:56.980]   - So the chronology here is super interesting, right?
[00:06:56.980 --> 00:07:01.040]   Because you started your PhD in 2017.
[00:07:01.040 --> 00:07:03.460]   You just got it in 2022, about a year ago.
[00:07:03.460 --> 00:07:04.300]   - Yeah, that's right.
[00:07:04.300 --> 00:07:08.340]   - So that overlapped with your work on monolith AI.
[00:07:08.340 --> 00:07:10.800]   And then you also started Human Loop in 2020.
[00:07:10.800 --> 00:07:13.420]   So just take me through that interesting journey.
[00:07:13.420 --> 00:07:16.040]   - So I wouldn't recommend this by the way.
[00:07:16.040 --> 00:07:18.920]   Like I'm a big advocate of focus within Human Loop.
[00:07:18.920 --> 00:07:20.560]   We try to be very focused,
[00:07:20.560 --> 00:07:22.880]   but I also just always had this itch
[00:07:22.880 --> 00:07:24.780]   to be part of companies and building things.
[00:07:24.780 --> 00:07:27.960]   And to be fair, I think it helped as a researcher
[00:07:27.960 --> 00:07:30.320]   because it gave you tangible real world problems
[00:07:30.320 --> 00:07:31.140]   and experience.
[00:07:31.140 --> 00:07:33.240]   I think in academia, it's really easy otherwise
[00:07:33.240 --> 00:07:35.840]   to just work on things that seem interesting to you,
[00:07:35.840 --> 00:07:37.820]   but maybe don't have such a big impact.
[00:07:37.820 --> 00:07:40.440]   So the way it came about, I was in the PhD
[00:07:40.440 --> 00:07:42.760]   and a very close friend, Richard Alfeld,
[00:07:42.760 --> 00:07:44.600]   who's now the CEO and founder of Monolith AI.
[00:07:44.600 --> 00:07:47.300]   They're a series A, almost series B company.
[00:07:47.300 --> 00:07:49.680]   And he was starting this company, came to me and he said,
[00:07:49.680 --> 00:07:51.720]   "I need someone who's on the ML side.
[00:07:51.720 --> 00:07:53.880]   "Just whilst I'm getting started, can you help out?"
[00:07:53.880 --> 00:07:54.720]   And so it was meant to be
[00:07:54.720 --> 00:07:56.540]   this very short term thing initially.
[00:07:56.540 --> 00:07:57.520]   I got sucked into it.
[00:07:57.520 --> 00:07:59.080]   I was spending at least a day a week,
[00:07:59.080 --> 00:08:02.360]   if not more of my PhD early on, but it was really fun.
[00:08:02.360 --> 00:08:03.800]   We were sitting in the offices of McLaren.
[00:08:03.800 --> 00:08:04.760]   They were our first customer.
[00:08:04.760 --> 00:08:06.120]   I think Airbus was an early customer.
[00:08:06.120 --> 00:08:07.800]   I helped hire the early team.
[00:08:07.800 --> 00:08:09.440]   And it was a really good experience
[00:08:09.440 --> 00:08:12.560]   of trying to do machine learning in the real world
[00:08:12.560 --> 00:08:14.060]   in high stakes situations, right?
[00:08:14.060 --> 00:08:15.640]   Physical engineering and understanding
[00:08:15.640 --> 00:08:16.960]   what did and didn't work.
[00:08:16.960 --> 00:08:18.560]   So I'm really glad I got that experience
[00:08:18.560 --> 00:08:21.660]   and it made me much more excited about starting a company.
[00:08:21.660 --> 00:08:23.080]   But that was still a part-time thing.
[00:08:23.080 --> 00:08:26.360]   And I think my supervisors sort of knew I was doing it,
[00:08:26.360 --> 00:08:29.320]   but it was a low enough time commitment that I could hide
[00:08:29.320 --> 00:08:32.080]   and still be focused on my PhD most of the time.
[00:08:32.080 --> 00:08:33.440]   With Human Loop, it was different.
[00:08:33.440 --> 00:08:34.680]   Like the way Human Loop came about,
[00:08:34.680 --> 00:08:37.440]   I came back from doing my internship at Google
[00:08:37.440 --> 00:08:41.040]   in Mountain View and doing the internship at Google
[00:08:41.040 --> 00:08:42.240]   convinced me that I loved Google,
[00:08:42.240 --> 00:08:43.720]   but I didn't want to work there in the near term.
[00:08:43.720 --> 00:08:45.720]   I wanted to be working in a space
[00:08:45.720 --> 00:08:47.160]   where there was a lot more urgency,
[00:08:47.160 --> 00:08:48.920]   where it felt existential, where we were all
[00:08:48.920 --> 00:08:52.400]   focused on the same problem as a team pulling together.
[00:08:52.400 --> 00:08:54.160]   And at Google, it just felt like you were
[00:08:54.160 --> 00:08:55.320]   part of something very big.
[00:08:55.320 --> 00:08:58.560]   I was surrounded by really smart, really capable people.
[00:08:58.560 --> 00:08:59.840]   I learned a lot from them.
[00:08:59.840 --> 00:09:01.480]   But the environment was more comfortable.
[00:09:01.480 --> 00:09:03.720]   And I wanted to be in a small--
[00:09:03.720 --> 00:09:05.440]   I wanted to be a startup, really.
[00:09:05.440 --> 00:09:07.280]   And so when I came back from Google,
[00:09:07.280 --> 00:09:09.020]   I sort of started thinking about ideas
[00:09:09.020 --> 00:09:10.840]   and speaking to the smartest people
[00:09:10.840 --> 00:09:13.240]   I knew to kind of see whether we could do something
[00:09:13.240 --> 00:09:14.560]   for when I finished the PhD.
[00:09:14.560 --> 00:09:15.360]   That was the point.
[00:09:15.360 --> 00:09:16.680]   I was just doing research.
[00:09:16.680 --> 00:09:19.080]   But Peter Jordan and I started working together
[00:09:19.080 --> 00:09:19.920]   in that process.
[00:09:19.920 --> 00:09:21.920]   We were all at a similar stage of kind of trying
[00:09:21.920 --> 00:09:23.280]   to find other people we might want
[00:09:23.280 --> 00:09:24.480]   to work on side projects with.
[00:09:24.480 --> 00:09:27.800]   And one of the side projects basically became Human Loop.
[00:09:27.800 --> 00:09:28.840]   And we got into YC.
[00:09:28.840 --> 00:09:30.420]   And we were like, OK, well, this is a great opportunity.
[00:09:30.420 --> 00:09:31.800]   Let's go do it.
[00:09:31.800 --> 00:09:34.760]   And just kind of one domino fell after another.
[00:09:34.760 --> 00:09:37.400]   And so didn't quite finish the PhD,
[00:09:37.400 --> 00:09:40.040]   but had enough research that I probably
[00:09:40.040 --> 00:09:41.480]   could have been writing up.
[00:09:41.480 --> 00:09:43.520]   And so at some point, I got an email from UCL.
[00:09:43.520 --> 00:09:46.100]   And they were like, if you don't submit in the next whatever--
[00:09:46.100 --> 00:09:46.800]   I think it was two months--
[00:09:46.800 --> 00:09:47.440]   It expires.
[00:09:47.440 --> 00:09:48.880]   --then it expires.
[00:09:48.880 --> 00:09:52.200]   And I almost didn't do it because obviously running
[00:09:52.200 --> 00:09:53.600]   a startup is such a full-time gig.
[00:09:53.600 --> 00:09:55.560]   But I had invested a lot of time.
[00:09:55.560 --> 00:09:58.280]   The honest reason why I did it is two things.
[00:09:58.280 --> 00:10:00.400]   One, my grandfather, who recently passed away,
[00:10:00.400 --> 00:10:02.480]   had just really wanted to see me finish.
[00:10:02.480 --> 00:10:05.360]   And so probably not super rational,
[00:10:05.360 --> 00:10:07.440]   but I just wanted to do that for that reason.
[00:10:07.440 --> 00:10:09.440]   But the other is I really love teaching.
[00:10:09.440 --> 00:10:12.280]   When I was a PhD student, I did a lot of TAing.
[00:10:12.280 --> 00:10:14.120]   I TAed the courses at the Gatsby,
[00:10:14.120 --> 00:10:16.040]   which is the institute that Jeff Hinton started
[00:10:16.040 --> 00:10:17.320]   when he was there.
[00:10:17.320 --> 00:10:18.760]   And I really enjoyed that.
[00:10:18.760 --> 00:10:20.160]   And I just knew that having a PhD
[00:10:20.160 --> 00:10:22.240]   would make it easier one day to come back to that.
[00:10:22.240 --> 00:10:25.280]   If I want to do a little bit of teaching at a university,
[00:10:25.280 --> 00:10:26.880]   having that title helps.
[00:10:26.880 --> 00:10:28.400]   I don't know if they have adjunct appointments here
[00:10:28.400 --> 00:10:29.720]   or maybe lecturer appointments.
[00:10:29.720 --> 00:10:30.600]   Yeah, something like that.
[00:10:30.600 --> 00:10:32.760]   I can't imagine doing it whilst running the startup,
[00:10:32.760 --> 00:10:34.000]   but afterwards.
[00:10:34.000 --> 00:10:36.800]   I've always wondered if I can give back in some shape
[00:10:36.800 --> 00:10:37.560]   or form.
[00:10:37.560 --> 00:10:39.400]   But maybe you might with your podcast
[00:10:39.400 --> 00:10:40.920]   when you get that started.
[00:10:40.920 --> 00:10:42.560]   What was the original pitch for Human Loop?
[00:10:42.560 --> 00:10:44.440]   You said it grew out of a side project.
[00:10:44.440 --> 00:10:46.560]   Yeah, so when we started Human Loop,
[00:10:46.560 --> 00:10:49.120]   both Peter, Jordan, and I had this strong conviction
[00:10:49.120 --> 00:10:51.320]   about the fact that NLP was getting phenomenally better.
[00:10:51.320 --> 00:10:54.240]   This was before GPT-3, but after BERT,
[00:10:54.240 --> 00:10:56.960]   and after transfer learning had really started to work
[00:10:56.960 --> 00:10:59.600]   for NLP, that you could pre-train a large language model
[00:10:59.600 --> 00:11:02.320]   on an unlabeled corpus and really quickly adapt it
[00:11:02.320 --> 00:11:03.160]   to new situations.
[00:11:03.160 --> 00:11:04.560]   That was new for NLP.
[00:11:04.560 --> 00:11:06.600]   And was it GPT-1 and 2?
[00:11:06.600 --> 00:11:07.600]   Or just BERT?
[00:11:07.600 --> 00:11:08.760]   GPT-1 and 2, it happened.
[00:11:08.760 --> 00:11:10.360]   But we were thinking about BERT.
[00:11:10.360 --> 00:11:13.200]   We were thinking about ULM-FIT as the first milestones that
[00:11:13.200 --> 00:11:14.920]   showed that this was possible.
[00:11:14.920 --> 00:11:17.680]   And it was very clear that as a result,
[00:11:17.680 --> 00:11:21.400]   there was going to be a huge wave of new, useful
[00:11:21.400 --> 00:11:24.000]   applications that enterprises could build on NLP that
[00:11:24.000 --> 00:11:26.560]   weren't previously possible, but that there was still
[00:11:26.560 --> 00:11:28.440]   a huge lack of technical expertise,
[00:11:28.440 --> 00:11:31.600]   and annotated data was still a big bottleneck.
[00:11:31.600 --> 00:11:35.200]   So we were always trying to make it a lot easier for developers
[00:11:35.200 --> 00:11:39.080]   and for companies to adopt NLP and build useful AI products.
[00:11:39.080 --> 00:11:42.120]   But at the time that we started, the bottleneck was mostly,
[00:11:42.120 --> 00:11:43.720]   OK, do you have the right ML expertise,
[00:11:43.720 --> 00:11:45.520]   and can you get enough annotated data?
[00:11:45.520 --> 00:11:47.520]   And so those were the problems we were initially
[00:11:47.520 --> 00:11:48.640]   helping people solve.
[00:11:48.640 --> 00:11:51.000]   And when GPT-3 came out, I wrote a blog post
[00:11:51.000 --> 00:11:52.160]   about this at the time.
[00:11:52.160 --> 00:11:54.000]   It was very clear that this was going to be the future,
[00:11:54.000 --> 00:11:56.080]   that actually, because in-context learning was
[00:11:56.080 --> 00:11:58.880]   starting to work, the amount of annotated data you would need
[00:11:58.880 --> 00:12:00.480]   was going to go down a lot.
[00:12:00.480 --> 00:12:02.360]   But until the InstructGPT papers,
[00:12:02.360 --> 00:12:03.840]   it still didn't feel practical.
[00:12:03.840 --> 00:12:07.440]   But after InstructGPT came out, once you've kind of mentally
[00:12:07.440 --> 00:12:09.000]   done that shift, it's very hard to keep
[00:12:09.000 --> 00:12:10.640]   working on anything else.
[00:12:10.640 --> 00:12:14.560]   And so a little over a year ago, we pivoted.
[00:12:14.560 --> 00:12:17.080]   And that was scary, because we had a thing that was working.
[00:12:17.080 --> 00:12:18.280]   We had paying customers.
[00:12:18.280 --> 00:12:19.360]   It was growing reasonably.
[00:12:19.360 --> 00:12:20.480]   We'd raised money.
[00:12:20.480 --> 00:12:22.920]   And we went to our investors at the time.
[00:12:22.920 --> 00:12:24.520]   And I remember having a conversation.
[00:12:24.520 --> 00:12:25.800]   We did a market size estimate.
[00:12:25.800 --> 00:12:27.560]   I actually filled out the YC application,
[00:12:27.560 --> 00:12:28.600]   because I think the YC application is
[00:12:28.600 --> 00:12:30.480]   like the simplest business model you could possibly build,
[00:12:30.480 --> 00:12:30.640]   right?
[00:12:30.640 --> 00:12:31.280]   What are you going to build?
[00:12:31.280 --> 00:12:31.880]   Who is it for?
[00:12:31.880 --> 00:12:32.640]   How are you going to make money?
[00:12:32.640 --> 00:12:33.880]   How big is the market?
[00:12:33.880 --> 00:12:35.640]   And I did the market size question.
[00:12:35.640 --> 00:12:36.840]   And at the time, we did it.
[00:12:36.840 --> 00:12:39.280]   And I was like, I reckon there are maybe three or maybe 400
[00:12:39.280 --> 00:12:41.800]   companies in the world who might need a product like this.
[00:12:41.800 --> 00:12:44.280]   And the assumption was that, OK, it's tiny today.
[00:12:44.280 --> 00:12:45.880]   It's mostly a small number of startups.
[00:12:45.880 --> 00:12:47.840]   But it will be huge in the future.
[00:12:47.840 --> 00:12:49.880]   And that turned out clearly to be right.
[00:12:49.880 --> 00:12:52.200]   I didn't realize how quickly it would happen.
[00:12:52.200 --> 00:12:52.700]   Yeah.
[00:12:52.700 --> 00:12:55.560]   It's obviously surprised, I think, a lot of us.
[00:12:55.560 --> 00:12:58.600]   But you were paying attention to the research
[00:12:58.600 --> 00:13:01.640]   when I guess a lot of people were not necessarily
[00:13:01.640 --> 00:13:03.840]   looking at that.
[00:13:03.840 --> 00:13:07.960]   To my understanding, you didn't have previous NLP knowledge
[00:13:07.960 --> 00:13:09.400]   or background, right?
[00:13:09.400 --> 00:13:11.960]   You did speech synthesis.
[00:13:11.960 --> 00:13:12.960]   I did speech synthesis.
[00:13:12.960 --> 00:13:15.440]   I did fundamental methods in deep learning.
[00:13:15.440 --> 00:13:16.920]   So you weren't specialized in any--
[00:13:16.920 --> 00:13:17.800]   I wasn't specialized.
[00:13:17.800 --> 00:13:20.440]   I was working on generative models, variational inference.
[00:13:20.440 --> 00:13:21.280]   I would actually say that--
[00:13:21.280 --> 00:13:23.760]   How did you know this was the thing to focus on, right?
[00:13:23.760 --> 00:13:25.240]   Well, so the interesting thing is
[00:13:25.240 --> 00:13:27.840]   that you don't need any NLP expertise
[00:13:27.840 --> 00:13:30.880]   to have gotten the current wave of deep learning or machine
[00:13:30.880 --> 00:13:31.380]   learning.
[00:13:31.380 --> 00:13:33.840]   So anything, I think, having previous NLP expertise
[00:13:33.840 --> 00:13:35.240]   is almost a disadvantage.
[00:13:35.240 --> 00:13:38.440]   I took an NLP course in my master's course.
[00:13:38.440 --> 00:13:40.520]   Fantastic lecturer, fantastic group.
[00:13:40.520 --> 00:13:43.560]   But at the time, there was only one lecture on deep learning.
[00:13:43.560 --> 00:13:47.640]   And this was 2016, 2017 or something, or 2015, 2016.
[00:13:47.640 --> 00:13:52.280]   The NLP community was still just waking up to the fact
[00:13:52.280 --> 00:13:54.480]   that deep learning was going to change everything.
[00:13:54.480 --> 00:13:57.040]   And the amazing thing about most machine learning attributes
[00:13:57.040 --> 00:13:58.620]   is it's another example of the bitter lesson
[00:13:58.620 --> 00:14:00.120]   that we were talking about earlier, right?
[00:14:00.120 --> 00:14:02.720]   General purpose learning methods at scale
[00:14:02.720 --> 00:14:05.080]   with large volumes of compute and data
[00:14:05.080 --> 00:14:06.960]   are often better than specialist systems.
[00:14:06.960 --> 00:14:08.880]   So if you understand that really well,
[00:14:08.880 --> 00:14:10.960]   you're probably at an advantage to someone
[00:14:10.960 --> 00:14:12.720]   who only understands the NLP side
[00:14:12.720 --> 00:14:14.440]   but doesn't understand that.
[00:14:14.440 --> 00:14:16.280]   I don't know if it's understanding so much as
[00:14:16.280 --> 00:14:16.780]   believe.
[00:14:16.780 --> 00:14:18.640]   [LAUGHTER]
[00:14:18.640 --> 00:14:19.520]   I think you're right.
[00:14:19.520 --> 00:14:20.360]   It's a bit of both.
[00:14:20.360 --> 00:14:21.360]   [LAUGHTER]
[00:14:21.360 --> 00:14:23.720]   Because I don't even think we fully understand it today.
[00:14:23.720 --> 00:14:24.680]   One leads to the other.
[00:14:24.680 --> 00:14:25.400]   Yes.
[00:14:25.400 --> 00:14:26.040]   Yes.
[00:14:26.040 --> 00:14:28.440]   That you take the evidence seriously.
[00:14:28.440 --> 00:14:28.960]   Yeah.
[00:14:28.960 --> 00:14:31.480]   And then you extend it out, and it still works.
[00:14:31.480 --> 00:14:33.360]   And you just keep going.
[00:14:33.360 --> 00:14:34.280]   Yeah, absolutely.
[00:14:34.280 --> 00:14:38.320]   So you got the time size wrong on the positive side.
[00:14:38.320 --> 00:14:39.760]   At the time, we were right, I think.
[00:14:39.760 --> 00:14:40.260]   Really?
[00:14:40.260 --> 00:14:41.240]   OK, yeah, yeah.
[00:14:41.240 --> 00:14:42.880]   And I know we were roughly right,
[00:14:42.880 --> 00:14:45.520]   because we were spending a lot of time speaking to OpenAI.
[00:14:45.520 --> 00:14:47.760]   And we were asking them, like, how many--
[00:14:47.760 --> 00:14:50.160]   they were sending us customers, and we were discussing it.
[00:14:50.160 --> 00:14:51.440]   We were asking about API usage, like,
[00:14:51.440 --> 00:14:52.860]   how many big companies are there?
[00:14:52.860 --> 00:14:54.880]   And there was a small number at the time.
[00:14:54.880 --> 00:14:56.560]   But it just rocketed since then.
[00:14:56.560 --> 00:14:57.320]   OK.
[00:14:57.320 --> 00:15:01.360]   So you were planning to build very closely in partnership
[00:15:01.360 --> 00:15:02.720]   with OpenAI.
[00:15:02.720 --> 00:15:04.960]   And we've always tried to keep close partnerships
[00:15:04.960 --> 00:15:06.880]   with all of the large language model providers.
[00:15:06.880 --> 00:15:10.200]   It's very clear that, whilst open source is fantastic,
[00:15:10.200 --> 00:15:13.080]   the very frontier is within private companies.
[00:15:13.080 --> 00:15:14.520]   And they are building the platforms
[00:15:14.520 --> 00:15:16.400]   that the rest of us are building on top of.
[00:15:16.400 --> 00:15:18.320]   And so not OpenAI specifically.
[00:15:18.320 --> 00:15:20.240]   Like, we're model and platform agnostic.
[00:15:20.240 --> 00:15:23.120]   But we want to help developers build useful applications
[00:15:23.120 --> 00:15:24.240]   with large language models.
[00:15:24.240 --> 00:15:25.760]   Whether that's OpenAI, or Cohere,
[00:15:25.760 --> 00:15:27.300]   or an open source model, or Anthropic,
[00:15:27.300 --> 00:15:28.280]   we don't mind.
[00:15:28.280 --> 00:15:30.640]   But being close to the model providers,
[00:15:30.640 --> 00:15:32.520]   make it easier for their customers to succeed,
[00:15:32.520 --> 00:15:33.680]   benefits them.
[00:15:33.680 --> 00:15:35.760]   And then we also get to learn from them
[00:15:35.760 --> 00:15:37.680]   about what problems people are facing,
[00:15:37.680 --> 00:15:39.440]   what they're planning to do in the future.
[00:15:39.440 --> 00:15:41.760]   So I think that all of the large language model providers
[00:15:41.760 --> 00:15:43.680]   are investing a lot in developer ecosystem,
[00:15:43.680 --> 00:15:45.040]   and not just being close to HumanLoop,
[00:15:45.040 --> 00:15:47.680]   but to anyone else who's making it easier for their customers.
[00:15:47.680 --> 00:15:48.080]   Yeah.
[00:15:48.080 --> 00:15:48.600]   Awesome.
[00:15:48.600 --> 00:15:49.100]   OK.
[00:15:49.100 --> 00:15:50.440]   So you started the company.
[00:15:50.440 --> 00:15:52.720]   How did you split things between the co-founders?
[00:15:52.720 --> 00:15:53.960]   It happened very organically.
[00:15:53.960 --> 00:15:56.360]   Remember, all on paper, we look really similar.
[00:15:56.360 --> 00:15:58.860]   Peter also has a PhD in machine learning, amazing engineer,
[00:15:58.860 --> 00:16:00.360]   previously been a CTO.
[00:16:00.360 --> 00:16:02.160]   Jordan has a master's in machine learning,
[00:16:02.160 --> 00:16:04.560]   he's a really good engineer as well.
[00:16:04.560 --> 00:16:05.880]   As we came to work on it, it just
[00:16:05.880 --> 00:16:08.480]   turned out we had natural strengths and interests that
[00:16:08.480 --> 00:16:09.800]   happened very, very organically.
[00:16:09.800 --> 00:16:11.360]   So Jordan is the kind of person who's
[00:16:11.360 --> 00:16:12.720]   got an amazing taste for product.
[00:16:12.720 --> 00:16:14.200]   He just notices things day to day.
[00:16:14.200 --> 00:16:16.600]   If he finds a product experience he really likes,
[00:16:16.600 --> 00:16:17.640]   you see his eyes light up.
[00:16:17.640 --> 00:16:19.640]   He's paying attention to it all the time.
[00:16:19.640 --> 00:16:21.680]   And so it made a lot of sense that he, over time,
[00:16:21.680 --> 00:16:24.240]   gravitated towards user experience, the design,
[00:16:24.240 --> 00:16:25.920]   actually thinking through the developer experience,
[00:16:25.920 --> 00:16:27.920]   and leading on product.
[00:16:27.920 --> 00:16:31.800]   Peter's got phenomenal stamina, and amazing engineering
[00:16:31.800 --> 00:16:33.760]   knowledge, and amazing attention to detail,
[00:16:33.760 --> 00:16:36.220]   and naturally gravitated towards taking on leading
[00:16:36.220 --> 00:16:37.440]   the engineering team.
[00:16:37.440 --> 00:16:38.320]   And I like doing this.
[00:16:38.320 --> 00:16:39.880]   I like chatting to people on podcasts.
[00:16:39.880 --> 00:16:42.360]   I like speaking to customers a lot.
[00:16:42.360 --> 00:16:44.200]   That's probably my favorite part of the job.
[00:16:44.200 --> 00:16:46.680]   And so naturally, I kind of ended up doing more of that
[00:16:46.680 --> 00:16:47.400]   work.
[00:16:47.400 --> 00:16:49.880]   But it wasn't that we sat down initially and said, OK,
[00:16:49.880 --> 00:16:52.320]   you're going to be the person who does sales and invest--
[00:16:52.320 --> 00:16:53.800]   it was much more organic than that.
[00:16:53.800 --> 00:16:54.360]   Yeah, yeah.
[00:16:54.360 --> 00:16:55.040]   Awesome.
[00:16:55.040 --> 00:16:56.500]   And you had to pick your customers.
[00:16:56.500 --> 00:16:58.280]   So what did you end up picking?
[00:16:58.280 --> 00:17:01.640]   So in the end, our customer changed dramatically
[00:17:01.640 --> 00:17:03.880]   when we launched the latest iteration of HumaLoop.
[00:17:03.880 --> 00:17:06.560]   When we decided to focus much more on large language models,
[00:17:06.560 --> 00:17:08.680]   we suddenly went from a world in which we're building
[00:17:08.680 --> 00:17:10.840]   predominantly for machine learning engineers, people
[00:17:10.840 --> 00:17:13.840]   who knew a lot about ML, maybe had research backgrounds,
[00:17:13.840 --> 00:17:16.080]   to building for generalist software engineers who
[00:17:16.080 --> 00:17:17.440]   are much more product-focused.
[00:17:17.440 --> 00:17:18.040]   Yeah, engineers.
[00:17:18.040 --> 00:17:19.600]   Something that some people, I think,
[00:17:19.600 --> 00:17:21.320]   would refer to as an AI engineer.
[00:17:21.320 --> 00:17:22.400]   So I've heard.
[00:17:22.400 --> 00:17:24.440]   And these are people who are much more
[00:17:24.440 --> 00:17:26.760]   focused on the outcome, on the product experience,
[00:17:26.760 --> 00:17:28.280]   on building something useful.
[00:17:28.280 --> 00:17:31.640]   And they're much more ambivalent towards the means
[00:17:31.640 --> 00:17:32.800]   that achieve the end.
[00:17:32.800 --> 00:17:34.840]   And that works out as a much better customer for a tooling
[00:17:34.840 --> 00:17:36.600]   provider as well, because they don't fight you
[00:17:36.600 --> 00:17:37.680]   to build everything themselves.
[00:17:37.680 --> 00:17:38.600]   They want good tools.
[00:17:38.600 --> 00:17:40.040]   And they're happy to pay for them,
[00:17:40.040 --> 00:17:41.960]   because they're trying to get to a good outcome
[00:17:41.960 --> 00:17:43.360]   as quickly as possible.
[00:17:43.360 --> 00:17:46.600]   So we found a much better reception amongst that audience
[00:17:46.600 --> 00:17:48.760]   and also that we could add a lot more value to them,
[00:17:48.760 --> 00:17:51.760]   because we could bake in best practices and knowledge we had.
[00:17:51.760 --> 00:17:53.600]   And that would make their lives much easier.
[00:17:53.600 --> 00:17:56.080]   And they didn't need to know so much about machine learning.
[00:17:56.080 --> 00:17:57.240]   Where do you find them?
[00:17:57.240 --> 00:17:59.440]   Because this was in early 2021.
[00:17:59.440 --> 00:18:00.200]   Yeah.
[00:18:00.200 --> 00:18:02.960]   There were no chat GPT forums.
[00:18:02.960 --> 00:18:05.800]   It wasn't a widely discussed topic on Twitter.
[00:18:05.800 --> 00:18:08.360]   Where do you find these early adopter types?
[00:18:08.360 --> 00:18:11.000]   So we could see some people using GPT-3.
[00:18:11.000 --> 00:18:13.080]   And so we would directly reach out to companies
[00:18:13.080 --> 00:18:14.840]   that were building on GPT-3.
[00:18:14.840 --> 00:18:17.440]   And in the early days, when we first did it,
[00:18:17.440 --> 00:18:20.200]   before we did the pivot, we gave ourselves a two-week sales
[00:18:20.200 --> 00:18:20.760]   experiment.
[00:18:20.760 --> 00:18:23.200]   We said, let's take our designs and our initial idea,
[00:18:23.200 --> 00:18:26.160]   and let's see if we can get 10 paying customers in two weeks.
[00:18:26.160 --> 00:18:27.480]   And on the second day, we had 10--
[00:18:27.480 --> 00:18:28.800]   Paying for what, specifically?
[00:18:28.800 --> 00:18:31.800]   So we were just pitching them on being part of a development
[00:18:31.800 --> 00:18:32.400]   partnership.
[00:18:32.400 --> 00:18:34.640]   So we said, we're building a tool that
[00:18:34.640 --> 00:18:36.240]   will help you with prompt engineering
[00:18:36.240 --> 00:18:38.560]   and evaluating how good your prompts are.
[00:18:38.560 --> 00:18:39.800]   This is what it looks like.
[00:18:39.800 --> 00:18:41.360]   We're looking for design partners.
[00:18:41.360 --> 00:18:43.680]   It costs this much to be a design partner.
[00:18:43.680 --> 00:18:46.040]   And on the second day, we already had 10.
[00:18:46.040 --> 00:18:46.720]   Amazing.
[00:18:46.720 --> 00:18:48.520]   And so we were like, OK, there's a real problem here.
[00:18:48.520 --> 00:18:50.120]   Because people were feeling the pain.
[00:18:50.120 --> 00:18:51.960]   And they were showing us their jerry-rigged solutions
[00:18:51.960 --> 00:18:52.720]   for this.
[00:18:52.720 --> 00:18:54.840]   They were showing us how they would stitch together
[00:18:54.840 --> 00:18:57.760]   Excel spreadsheets, and Grafana, and Mixpanel,
[00:18:57.760 --> 00:19:00.280]   and the OpenAI Playground in these very
[00:19:00.280 --> 00:19:03.560]   kludgy pipelines to somehow quickly iterate on prompts,
[00:19:03.560 --> 00:19:05.720]   version them, collaborate as a team,
[00:19:05.720 --> 00:19:08.640]   find some way to measure things that were very subjective.
[00:19:08.640 --> 00:19:10.440]   And so we were like, OK, actually, there's
[00:19:10.440 --> 00:19:12.400]   a very clear need here.
[00:19:12.400 --> 00:19:13.720]   Let's go help these people.
[00:19:13.720 --> 00:19:14.520]   Yeah, excellent.
[00:19:14.520 --> 00:19:16.800]   So what is HumanLoop today?
[00:19:16.800 --> 00:19:18.920]   Yeah, so at its core, we help engineers
[00:19:18.920 --> 00:19:23.440]   to measure and optimize LLM applications,
[00:19:23.440 --> 00:19:26.440]   in particular, helping them do prompt engineering, management,
[00:19:26.440 --> 00:19:27.720]   and then evaluation.
[00:19:27.720 --> 00:19:29.360]   So evaluation is particularly difficult
[00:19:29.360 --> 00:19:31.040]   for large language models, because they
[00:19:31.040 --> 00:19:33.160]   tend to be used for much more subjective applications
[00:19:33.160 --> 00:19:35.080]   than traditional machine learning, definitely
[00:19:35.080 --> 00:19:36.080]   than traditional software.
[00:19:36.080 --> 00:19:38.600]   If you're coming from a pure software, non-ML background,
[00:19:38.600 --> 00:19:39.760]   then the first thing you have to learn
[00:19:39.760 --> 00:19:41.100]   when you start working with LLMs
[00:19:41.100 --> 00:19:44.400]   is this stuff is stochastic, which I think most people are
[00:19:44.400 --> 00:19:45.080]   not used to.
[00:19:45.080 --> 00:19:47.160]   So just playing with software that every time you run it
[00:19:47.160 --> 00:19:49.360]   is different, and you can't just write unit tests,
[00:19:49.360 --> 00:19:51.480]   is the first kind of painful lesson.
[00:19:51.480 --> 00:19:53.960]   But then it turns out that a big piece of these applications
[00:19:53.960 --> 00:19:55.240]   ends up in prompts.
[00:19:55.240 --> 00:19:57.280]   And these are natural language instructions,
[00:19:57.280 --> 00:19:59.440]   but they're having similar impact to what code has.
[00:19:59.440 --> 00:20:01.720]   So they need to be treated with the importance of code.
[00:20:01.720 --> 00:20:04.560]   And so iterating on that, managing it, versioning it,
[00:20:04.560 --> 00:20:06.360]   evaluating it, those are the problems
[00:20:06.360 --> 00:20:08.360]   that HumanLoop helps engineers with today.
[00:20:08.360 --> 00:20:10.020]   And in particular, we tend to be focused
[00:20:10.020 --> 00:20:13.480]   on companies that are at a certain scale,
[00:20:13.480 --> 00:20:16.320]   because one of the challenges that-- one,
[00:20:16.320 --> 00:20:17.880]   they tend to care more about evaluation.
[00:20:17.880 --> 00:20:19.000]   I think if you're a two-person startup,
[00:20:19.000 --> 00:20:20.540]   you sort of build something quick MVP
[00:20:20.540 --> 00:20:22.120]   and you YOLO it into production.
[00:20:22.120 --> 00:20:23.520]   But larger companies need to have
[00:20:23.520 --> 00:20:25.200]   some confidence of the product experience
[00:20:25.200 --> 00:20:26.680]   before they launch something.
[00:20:26.680 --> 00:20:29.200]   And also what we've found is that there's a lot more
[00:20:29.200 --> 00:20:31.840]   collaboration between engineers and non-engineers,
[00:20:31.840 --> 00:20:35.400]   between product managers and domain experts who
[00:20:35.400 --> 00:20:37.660]   are involved in the design, the prompt engineering,
[00:20:37.660 --> 00:20:40.460]   the evaluation, but are maybe not the engineering part.
[00:20:40.460 --> 00:20:42.040]   And they have to work together nicely.
[00:20:42.040 --> 00:20:44.440]   So giving them the right tools has been a really important
[00:20:44.440 --> 00:20:45.160]   part as well.
[00:20:45.160 --> 00:20:46.000]   Yeah.
[00:20:46.000 --> 00:20:47.760]   Something I've often talked about
[00:20:47.760 --> 00:20:50.560]   with other startups in this space is who is the buyer?
[00:20:50.560 --> 00:20:51.080]   Yeah.
[00:20:51.080 --> 00:20:52.640]   Because you talked about collaboration
[00:20:52.640 --> 00:20:56.080]   between the engineer and the PM or whoever.
[00:20:56.080 --> 00:20:58.760]   And it's not clear sometimes.
[00:20:58.760 --> 00:20:59.880]   Do you have a clear answer?
[00:20:59.880 --> 00:21:02.160]   It varies highly on company stage.
[00:21:02.160 --> 00:21:04.280]   So in the early days when we started HumanLoop,
[00:21:04.280 --> 00:21:05.680]   you said, where do we find our customers?
[00:21:05.680 --> 00:21:06.960]   They were all startups and scale-ups,
[00:21:06.960 --> 00:21:09.200]   because those were the only people building with GPT-3
[00:21:09.200 --> 00:21:10.200]   more than a year ago.
[00:21:10.200 --> 00:21:11.720]   There was no large companies.
[00:21:11.720 --> 00:21:13.960]   And there, it was always founder, CTO.
[00:21:13.960 --> 00:21:15.480]   Even if there were 10, 20-person companies,
[00:21:15.480 --> 00:21:17.000]   series A companies, always founder
[00:21:17.000 --> 00:21:18.840]   who was speaking to, reaching out to us,
[00:21:18.840 --> 00:21:19.880]   who was helping build it.
[00:21:19.880 --> 00:21:22.640]   So an example here, one of our earliest customers was Mem.
[00:21:22.640 --> 00:21:25.560]   And it was Dennis at Mem who was the person we were speaking to.
[00:21:25.560 --> 00:21:27.200]   Now that we're at a bit more at scale
[00:21:27.200 --> 00:21:29.000]   and we're speaking to larger companies,
[00:21:29.000 --> 00:21:30.440]   it's a little bit more varied.
[00:21:30.440 --> 00:21:33.960]   Surprisingly, it's still quite often senior management
[00:21:33.960 --> 00:21:35.520]   that first speaks to us.
[00:21:35.520 --> 00:21:38.040]   So with Duolingo, it was Severin, the CTO,
[00:21:38.040 --> 00:21:40.440]   was actually our first contact.
[00:21:40.440 --> 00:21:41.360]   Just inbound.
[00:21:41.360 --> 00:21:42.360]   Inbound.
[00:21:42.360 --> 00:21:45.240]   But increasingly now, it's people
[00:21:45.240 --> 00:21:48.080]   who are engineers who are actually working on projects.
[00:21:48.080 --> 00:21:50.920]   So it's like a senior staff engineer or something like that
[00:21:50.920 --> 00:21:52.640]   will reach out, book a demo.
[00:21:52.640 --> 00:21:54.440]   They'll probably sign up first and have a play.
[00:21:54.440 --> 00:21:55.760]   But then they tend to book a demo
[00:21:55.760 --> 00:21:58.360]   because they want to discuss data privacy
[00:21:58.360 --> 00:22:00.160]   and how things will be rolled out
[00:22:00.160 --> 00:22:03.120]   and going beyond just individual usage.
[00:22:03.120 --> 00:22:05.920]   But that's the usual flow, is we see them sign up.
[00:22:05.920 --> 00:22:06.880]   Sometimes we reach out to them.
[00:22:06.880 --> 00:22:08.400]   Often they'll reach out to us.
[00:22:08.400 --> 00:22:09.640]   And then the conversation starts.
[00:22:09.640 --> 00:22:10.520]   Yeah, yeah.
[00:22:10.520 --> 00:22:11.240]   Awesome.
[00:22:11.240 --> 00:22:14.360]   For people who want to get a better sense of Humaloop,
[00:22:14.360 --> 00:22:16.880]   the company, I think the website does a fantastic job
[00:22:16.880 --> 00:22:17.720]   of explaining it.
[00:22:17.720 --> 00:22:18.120]   Thank you.
[00:22:18.120 --> 00:22:19.320]   We're always working on it.
[00:22:19.320 --> 00:22:21.000]   It must have put in quite a lot of work.
[00:22:21.000 --> 00:22:23.720]   So it says here, Humaloop application platform
[00:22:23.720 --> 00:22:26.680]   includes a playground, monitoring, deployment,
[00:22:26.680 --> 00:22:29.640]   A/B testing, prop manager, evaluation, data store,
[00:22:29.640 --> 00:22:30.880]   and fine tuning.
[00:22:30.880 --> 00:22:32.240]   And based on our chat earlier, it
[00:22:32.240 --> 00:22:35.360]   seems like evaluation is kind of the more beta one that's
[00:22:35.360 --> 00:22:37.040]   in sort of like a private beta.
[00:22:37.040 --> 00:22:37.680]   That's correct, yeah.
[00:22:37.680 --> 00:22:39.080]   So we have evaluation and private.
[00:22:39.400 --> 00:22:41.320]   There's always been some aspect of evaluation.
[00:22:41.320 --> 00:22:43.200]   It was actually the first problem
[00:22:43.200 --> 00:22:44.840]   that we were solving for customers.
[00:22:44.840 --> 00:22:47.360]   But evaluation in Humaloop early on
[00:22:47.360 --> 00:22:50.720]   was driven entirely by end user feedback.
[00:22:50.720 --> 00:22:52.360]   So if you're building an LLM app,
[00:22:52.360 --> 00:22:53.960]   there's probably three different places
[00:22:53.960 --> 00:22:55.360]   where evaluation matters a lot.
[00:22:55.360 --> 00:22:57.040]   There's the evaluation that you need
[00:22:57.040 --> 00:22:59.320]   when you're iterating in design, and you haven't got something
[00:22:59.320 --> 00:23:00.320]   in production yet.
[00:23:00.320 --> 00:23:01.400]   But you just need feedback.
[00:23:01.400 --> 00:23:03.480]   As you're making changes, are you making things better?
[00:23:03.480 --> 00:23:04.640]   You're iterating on prompts.
[00:23:04.640 --> 00:23:07.320]   You're iterating on the context, trying out different models.
[00:23:07.320 --> 00:23:10.280]   How do you know that the changes are actually improving things?
[00:23:10.280 --> 00:23:12.680]   Then once you're in production, there's
[00:23:12.680 --> 00:23:15.640]   sort of a form of evaluation you need for monitoring.
[00:23:15.640 --> 00:23:17.440]   It seemed to work when I was in development,
[00:23:17.440 --> 00:23:19.280]   but now I'm putting a whole bunch of different customer
[00:23:19.280 --> 00:23:20.280]   inputs through it.
[00:23:20.280 --> 00:23:22.600]   Is it still performing the way that I expect it?
[00:23:22.600 --> 00:23:23.680]   And then the last one is something
[00:23:23.680 --> 00:23:26.140]   like equivalent to integration tests or something like this.
[00:23:26.140 --> 00:23:27.720]   Every time you make a change, how
[00:23:27.720 --> 00:23:30.100]   do you know you're not making it worse on the things that
[00:23:30.100 --> 00:23:31.000]   are already there?
[00:23:31.000 --> 00:23:33.080]   And so I think we always had a really good version
[00:23:33.080 --> 00:23:35.280]   of the monitoring user feedback version.
[00:23:35.280 --> 00:23:37.760]   But what we were missing was support for offline evaluation
[00:23:37.760 --> 00:23:39.840]   and being able to do evaluation during development
[00:23:39.840 --> 00:23:41.120]   or regression testing.
[00:23:41.120 --> 00:23:43.880]   And we're going to be launching something for that very soon.
[00:23:43.880 --> 00:23:45.880]   This is slightly unintuitive to me,
[00:23:45.880 --> 00:23:48.560]   because I would typically just assume all three
[00:23:48.560 --> 00:23:50.760]   are the same evals.
[00:23:50.760 --> 00:23:53.760]   Yeah, so they can't necessarily be the same evals just
[00:23:53.760 --> 00:23:58.480]   because you don't have the user feedback at the time
[00:23:58.480 --> 00:23:59.640]   that you're in development.
[00:23:59.640 --> 00:24:00.680]   I'm not thinking about user feedback.
[00:24:00.680 --> 00:24:02.100]   I'm just thinking about validating
[00:24:02.100 --> 00:24:03.280]   the output that you get.
[00:24:03.280 --> 00:24:04.720]   Yeah, so you're validating in similar ways.
[00:24:04.720 --> 00:24:06.560]   But if you're doing a really subjective task,
[00:24:06.560 --> 00:24:08.560]   then I think the only real ground truth
[00:24:08.560 --> 00:24:10.600]   is what your customers say is the right answer.
[00:24:10.600 --> 00:24:12.720]   If you're building copilot, do the customers
[00:24:12.720 --> 00:24:14.760]   accept the code suggestions or not?
[00:24:14.760 --> 00:24:16.920]   That is the thing that ultimately matters.
[00:24:16.920 --> 00:24:19.640]   And you can only have proxies for that in development.
[00:24:19.640 --> 00:24:22.400]   And so that was why those two things end up being different.
[00:24:22.400 --> 00:24:23.040]   Yeah.
[00:24:23.040 --> 00:24:26.520]   And in terms of the quality of feedback--
[00:24:26.520 --> 00:24:28.720]   so we did an episode with Amplitude,
[00:24:28.720 --> 00:24:32.520]   which is an analytics platform dedicated for collecting
[00:24:32.520 --> 00:24:34.280]   this kind of behavioral feedback.
[00:24:34.280 --> 00:24:35.760]   And you mentioned copilot.
[00:24:35.760 --> 00:24:38.000]   There was a very famous post about reverse engineering
[00:24:38.000 --> 00:24:41.200]   copilot that shows you the degree of feedback.
[00:24:41.200 --> 00:24:43.840]   I think typically when people implement these things,
[00:24:43.840 --> 00:24:45.840]   they implement it as a sort of thumbs up,
[00:24:45.840 --> 00:24:48.200]   thumbs down sort of binary feedback
[00:24:48.200 --> 00:24:52.280]   until you find that nobody does those feedback.
[00:24:52.280 --> 00:24:54.200]   I barely used the up/down on chat GPT.
[00:24:54.200 --> 00:24:56.040]   Yeah, so this was something we learned really
[00:24:56.040 --> 00:24:58.240]   early on in building Human Loop.
[00:24:58.240 --> 00:25:00.760]   And the feedback aspects of Human Loop
[00:25:00.760 --> 00:25:03.440]   were very customer-driven.
[00:25:03.440 --> 00:25:04.680]   So the people who were getting--
[00:25:04.680 --> 00:25:06.480]   amongst our early users, the people who were getting
[00:25:06.480 --> 00:25:09.000]   traction and who had built something that was working well
[00:25:09.000 --> 00:25:11.640]   had jerry-rigged some version of feedback collection
[00:25:11.640 --> 00:25:13.160]   and improvement themselves.
[00:25:13.160 --> 00:25:15.520]   And they were pushing for something better here.
[00:25:15.520 --> 00:25:18.440]   And they all were collecting usually three types
[00:25:18.440 --> 00:25:18.960]   of feedback.
[00:25:18.960 --> 00:25:20.480]   And Human Loop supports all three of these.
[00:25:20.480 --> 00:25:22.400]   So you have the thumbs up, thumbs down type feedback
[00:25:22.400 --> 00:25:23.760]   that you just described.
[00:25:23.760 --> 00:25:24.880]   You don't get much of it.
[00:25:24.880 --> 00:25:27.160]   It's useful when you get it, but you don't get that much.
[00:25:27.160 --> 00:25:28.280]   And then the other form of feedback--
[00:25:28.280 --> 00:25:29.560]   so we call that votes.
[00:25:29.560 --> 00:25:30.920]   And then you have actions.
[00:25:30.920 --> 00:25:33.720]   And these are the implicit signals of user feedback.
[00:25:33.720 --> 00:25:35.480]   So I can give a concrete example here.
[00:25:35.480 --> 00:25:37.560]   There's a company I really like called Pseudowrite.
[00:25:37.560 --> 00:25:39.640]   And Pseudowrite, founded by James Yu,
[00:25:39.640 --> 00:25:42.600]   and they're building an editor experience for fiction writers
[00:25:42.600 --> 00:25:43.360]   that helps them.
[00:25:43.360 --> 00:25:45.640]   So as they're writing their stories or novels,
[00:25:45.640 --> 00:25:46.480]   there's a sidebar.
[00:25:46.480 --> 00:25:47.840]   And you can highlight text.
[00:25:47.840 --> 00:25:50.040]   And you can say, help me come up with a different way
[00:25:50.040 --> 00:25:52.600]   of saying this or in a more evocative way.
[00:25:52.600 --> 00:25:54.520]   There's many different features built in.
[00:25:54.520 --> 00:25:57.840]   And they had built in early on analytics around,
[00:25:57.840 --> 00:25:59.520]   does the user accept a suggestion?
[00:25:59.520 --> 00:26:02.280]   Do they refresh and regenerate multiple times?
[00:26:02.280 --> 00:26:04.080]   How much do they edit the suggestion
[00:26:04.080 --> 00:26:05.880]   before including it in their story?
[00:26:05.880 --> 00:26:07.360]   Do they then share that?
[00:26:07.360 --> 00:26:09.200]   And all of those implicit signals
[00:26:09.200 --> 00:26:11.320]   correlate really well with the quality
[00:26:11.320 --> 00:26:12.440]   of the model or the prompt.
[00:26:12.440 --> 00:26:14.520]   And they were running experiments all the time
[00:26:14.520 --> 00:26:15.400]   to make these better.
[00:26:15.400 --> 00:26:17.920]   And you could just see it in their traction figures.
[00:26:17.920 --> 00:26:20.280]   As they figured out the right prompts, the right features,
[00:26:20.280 --> 00:26:22.280]   the things that people were actually including,
[00:26:22.280 --> 00:26:25.720]   the product became much more loved by their users.
[00:26:25.720 --> 00:26:26.480]   Was there a third?
[00:26:26.480 --> 00:26:27.360]   You said there was--
[00:26:27.360 --> 00:26:29.680]   And the third one is corrections.
[00:26:29.680 --> 00:26:31.480]   So this helps particularly when you
[00:26:31.480 --> 00:26:33.600]   want to do fine tuning later on.
[00:26:33.600 --> 00:26:36.480]   So anywhere you're exposing generated text to a user
[00:26:36.480 --> 00:26:38.360]   and they can edit it before using it,
[00:26:38.360 --> 00:26:39.400]   then that's worth logging.
[00:26:39.400 --> 00:26:41.860]   So a concrete example here is we have a couple of customers
[00:26:41.860 --> 00:26:43.240]   who do sales email generation.
[00:26:43.240 --> 00:26:44.480]   And they generate a draft.
[00:26:44.480 --> 00:26:45.600]   Someone edits it.
[00:26:45.600 --> 00:26:47.040]   And then they send the draft.
[00:26:47.040 --> 00:26:49.320]   And so they capture the edited drafts.
[00:26:49.320 --> 00:26:53.160]   And I think a lot of this is sort of preemptive.
[00:26:53.160 --> 00:26:56.400]   They don't necessarily use that captured data immediately.
[00:26:56.400 --> 00:26:59.480]   But it's there if they want it for fine tuning,
[00:26:59.480 --> 00:27:02.880]   for validating prompt changes, and anything like that.
[00:27:02.880 --> 00:27:03.720]   Exactly.
[00:27:03.720 --> 00:27:04.220]   Exactly.
[00:27:04.220 --> 00:27:05.600]   It's data that you want to have.
[00:27:05.600 --> 00:27:07.760]   And you want to have it in an accessible way such
[00:27:07.760 --> 00:27:09.340]   that you can improve things over time.
[00:27:09.340 --> 00:27:09.840]   Yeah.
[00:27:09.840 --> 00:27:13.200]   And you have a UI to expose it.
[00:27:13.200 --> 00:27:14.880]   But do you think that people use that UI?
[00:27:14.880 --> 00:27:18.480]   Or do they prefer to export it to, I don't know, Excel?
[00:27:18.480 --> 00:27:21.640]   Or how do people like to consume their data
[00:27:21.640 --> 00:27:23.280]   once they've captured it?
[00:27:23.280 --> 00:27:26.000]   Yeah, so we see a lot of people using it in the UI.
[00:27:26.000 --> 00:27:27.600]   And part of the reason for that is
[00:27:27.600 --> 00:27:29.240]   we have this bidirectional experience
[00:27:29.240 --> 00:27:30.520]   with an interactive playground.
[00:27:30.520 --> 00:27:32.360]   So we have the ability to take the data that
[00:27:32.360 --> 00:27:35.360]   was logged in production and open it back up
[00:27:35.360 --> 00:27:37.480]   in an environment where you can rerun the models when
[00:27:37.480 --> 00:27:38.600]   you make changes.
[00:27:38.600 --> 00:27:41.280]   And that ability has been really important for people
[00:27:41.280 --> 00:27:43.240]   to reason about counterfactuals.
[00:27:43.240 --> 00:27:45.040]   Oh, the model failed here.
[00:27:45.040 --> 00:27:47.280]   If the context retrieval had worked correctly,
[00:27:47.280 --> 00:27:48.520]   would the model have succeeded?
[00:27:48.520 --> 00:27:51.160]   And they can really immediately run that counterfactual.
[00:27:51.160 --> 00:27:54.000]   Or is it a problem with GPT 3.5 versus 4?
[00:27:54.000 --> 00:27:56.680]   So they'll run it with 4 and see, does that fix it?
[00:27:56.680 --> 00:27:58.360]   And that lets them build up an intuition
[00:27:58.360 --> 00:28:00.320]   about why things have worked or haven't worked.
[00:28:00.320 --> 00:28:02.920]   People do export data sometimes.
[00:28:02.920 --> 00:28:05.200]   So we allow people to format the data in the right way
[00:28:05.200 --> 00:28:06.720]   for fine tuning and then export it.
[00:28:06.720 --> 00:28:09.080]   And that's something we see people do quite a lot if they
[00:28:09.080 --> 00:28:10.520]   want to fine tune their own models.
[00:28:10.520 --> 00:28:12.560]   But we try to give fairly powerful data exploration
[00:28:12.560 --> 00:28:13.920]   tools within HumanLoop.
[00:28:13.920 --> 00:28:14.760]   Yeah.
[00:28:14.760 --> 00:28:17.260]   What about your integrations with the rest of the ecosystem?
[00:28:17.260 --> 00:28:19.360]   On your landing page, you have LangChain,
[00:28:19.360 --> 00:28:22.760]   all the GPTs mentioned, Chroma, Pinecone, Snowflake.
[00:28:22.760 --> 00:28:24.680]   And obviously, the LLM providers.
[00:28:24.680 --> 00:28:25.160]   Yeah.
[00:28:25.160 --> 00:28:27.280]   So the way we see HumanLoop is sitting
[00:28:27.280 --> 00:28:31.640]   between the base LLM providers and an orchestration framework
[00:28:31.640 --> 00:28:34.440]   like code, LangChain or Lama Index
[00:28:34.440 --> 00:28:36.960]   might sit separately to that.
[00:28:36.960 --> 00:28:39.560]   You have this analogy, I think, of LLM first or code
[00:28:39.560 --> 00:28:41.520]   first AI applications.
[00:28:41.520 --> 00:28:43.200]   And we're very strongly of the opinion
[00:28:43.200 --> 00:28:45.560]   that most things should be happening in code, right?
[00:28:45.560 --> 00:28:47.000]   That developers want to write code.
[00:28:47.000 --> 00:28:48.520]   They want to be able to orchestrate these things in
[00:28:48.520 --> 00:28:49.280]   code.
[00:28:49.280 --> 00:28:51.960]   But for the pieces that require LLMs,
[00:28:51.960 --> 00:28:53.080]   you do need separate tooling.
[00:28:53.080 --> 00:28:54.800]   You need the right tools for prompt engineering.
[00:28:54.800 --> 00:28:56.520]   You need some way to evaluate that.
[00:28:56.520 --> 00:28:58.680]   And so we want HumanLoop to plug in very nicely
[00:28:58.680 --> 00:29:00.960]   into all of these orchestration frameworks
[00:29:00.960 --> 00:29:02.800]   that you might be using or your own code
[00:29:02.800 --> 00:29:05.680]   and let you collect the prompts, the evaluation data
[00:29:05.680 --> 00:29:09.320]   that you need to iterate quickly in a nice UI.
[00:29:09.320 --> 00:29:12.040]   So here is where LangChain collides with you.
[00:29:12.040 --> 00:29:13.040]   Has started to now.
[00:29:13.040 --> 00:29:15.520]   Yes, because they just released the Prompt Manager.
[00:29:15.520 --> 00:29:16.380]   Yeah.
[00:29:16.380 --> 00:29:20.960]   And they also have a dashboard to observe and track and store
[00:29:20.960 --> 00:29:24.360]   their prompts and data and the results.
[00:29:24.360 --> 00:29:26.160]   They don't have feedback collection yet,
[00:29:26.160 --> 00:29:27.440]   but they're going to build it.
[00:29:27.440 --> 00:29:28.760]   I'm sure they will.
[00:29:28.760 --> 00:29:31.080]   You know, it's a very vibrant ecosystem.
[00:29:31.080 --> 00:29:33.560]   There's lots of people running after similar problems
[00:29:33.560 --> 00:29:36.000]   and listening to developers and building what they need.
[00:29:36.000 --> 00:29:39.440]   So I'm not completely surprised that they've
[00:29:39.440 --> 00:29:41.520]   ended up building some of the features
[00:29:41.520 --> 00:29:43.640]   that we have because I think so much of what we need
[00:29:43.640 --> 00:29:46.520]   is really important for developers to achieve stuff.
[00:29:46.520 --> 00:29:48.400]   I think one of the strongest parts of it
[00:29:48.400 --> 00:29:50.880]   is it's going to be very tightly integrated with LangChain.
[00:29:50.880 --> 00:29:52.960]   But a lot of people are not building on LangChain.
[00:29:52.960 --> 00:29:55.560]   And so for anyone for whom LangChain is not their production
[00:29:55.560 --> 00:29:58.880]   choice system, then I think actually it's
[00:29:58.880 --> 00:30:01.200]   going to be friction to work in that way.
[00:30:01.200 --> 00:30:04.600]   I think that there's going to be a plethora of different options
[00:30:04.600 --> 00:30:05.760]   for developers out there.
[00:30:05.760 --> 00:30:07.880]   And they'll find their own niches slightly.
[00:30:07.880 --> 00:30:10.360]   I think we're focused a little bit more, as I said,
[00:30:10.360 --> 00:30:12.680]   on companies where collaboration is very important,
[00:30:12.680 --> 00:30:16.280]   little bit larger scale, and slightly less so far
[00:30:16.280 --> 00:30:18.360]   as an individual developer than quite the same way
[00:30:18.360 --> 00:30:20.120]   that LangChain has been to date.
[00:30:20.120 --> 00:30:21.880]   That's a fair characterization, I think.
[00:30:21.880 --> 00:30:24.880]   It's funny because, yeah, you are more agnostic
[00:30:24.880 --> 00:30:25.960]   than LangChain is.
[00:30:25.960 --> 00:30:28.320]   And that is a strength of yours.
[00:30:28.320 --> 00:30:31.280]   But I've also worked for companies
[00:30:31.280 --> 00:30:35.080]   which have tried too hard to be Switzerland
[00:30:35.080 --> 00:30:37.080]   and to not be opinionated about anything.
[00:30:37.080 --> 00:30:39.120]   And it's bitten them in--
[00:30:39.120 --> 00:30:40.720]   You have to have opinions, right?
[00:30:40.720 --> 00:30:44.280]   You've got to bake into the-- we learn a lot from our customers.
[00:30:44.280 --> 00:30:46.160]   And then we try to productize those learnings.
[00:30:46.160 --> 00:30:48.200]   So I gave you a concrete example earlier
[00:30:48.200 --> 00:30:50.640]   on having good defaults for what types of feedback
[00:30:50.640 --> 00:30:51.640]   you can collect.
[00:30:51.640 --> 00:30:52.720]   And that's not an accident.
[00:30:52.720 --> 00:30:53.920]   We're very opinionated about that
[00:30:53.920 --> 00:30:56.160]   because we've seen what's worked for the people who
[00:30:56.160 --> 00:30:57.360]   are getting to good results.
[00:30:57.360 --> 00:30:59.440]   And now if you sort of set up human loop with that,
[00:30:59.440 --> 00:31:01.480]   you naturally end up with the correct defaults.
[00:31:01.480 --> 00:31:03.480]   And there's loads of examples of that throughout the product
[00:31:03.480 --> 00:31:05.280]   where we're feeding back learnings
[00:31:05.280 --> 00:31:08.200]   from having a very large range of customers in production
[00:31:08.200 --> 00:31:10.840]   to try and set up sensible defaults that you don't realize
[00:31:10.840 --> 00:31:13.200]   it, but we're nudging you towards doing the right thing.
[00:31:13.200 --> 00:31:13.840]   Yeah.
[00:31:13.840 --> 00:31:14.600]   Yeah.
[00:31:14.600 --> 00:31:15.240]   Excellent.
[00:31:15.240 --> 00:31:16.800]   So that's a really great overview
[00:31:16.800 --> 00:31:18.600]   of the product's surface area.
[00:31:18.600 --> 00:31:20.640]   I mean, I don't know if we left out anything
[00:31:20.640 --> 00:31:21.760]   that you want to highlight.
[00:31:21.760 --> 00:31:22.840]   No, I think that's great.
[00:31:22.840 --> 00:31:25.560]   And the focus for us, I think, being
[00:31:25.560 --> 00:31:28.280]   like a really excellent tool for prompt management,
[00:31:28.280 --> 00:31:31.040]   engineering, versioning, and also evaluation.
[00:31:31.040 --> 00:31:34.040]   So kind of combining those and making that easy for a team.
[00:31:34.040 --> 00:31:34.800]   Yeah.
[00:31:34.800 --> 00:31:37.320]   What's your estimate of the time now?
[00:31:37.320 --> 00:31:38.480]   Oh, god.
[00:31:38.480 --> 00:31:41.480]   I mean, eventually, at the current rate of growth,
[00:31:41.480 --> 00:31:43.520]   right, I think it's really difficult to--
[00:31:43.520 --> 00:31:44.880]   All known atoms in the universe.
[00:31:44.880 --> 00:31:45.400]   Yeah.
[00:31:45.400 --> 00:31:46.800]   It's difficult to put a size on it
[00:31:46.800 --> 00:31:48.320]   because how big it's going to be.
[00:31:48.320 --> 00:31:51.160]   Like, certainly, like more than large enough
[00:31:51.160 --> 00:31:52.400]   for a venture backable outcome.
[00:31:52.400 --> 00:31:54.100]   Today, I don't know, Datadog is something
[00:31:54.100 --> 00:31:57.000]   like a $35 billion company doing like web monitoring
[00:31:57.000 --> 00:31:57.560]   or whatever.
[00:31:57.560 --> 00:31:59.960]   I think LLMs and AI are going to be bigger than software.
[00:31:59.960 --> 00:32:00.460]   Yeah.
[00:32:00.460 --> 00:32:03.960]   And that market is going to be absolutely enormous.
[00:32:03.960 --> 00:32:06.480]   And so trying to put a size on the time
[00:32:06.480 --> 00:32:08.120]   feels a little silly almost.
[00:32:08.120 --> 00:32:08.640]   Oh, I know.
[00:32:08.640 --> 00:32:10.520]   You had to do it for your exercise.
[00:32:10.520 --> 00:32:12.400]   So I just figured I'd get an update.
[00:32:12.400 --> 00:32:14.160]   It was a different world back then, right?
[00:32:14.160 --> 00:32:15.640]   The way that I was doing it, trying
[00:32:15.640 --> 00:32:17.720]   to get people to take the idea of putting GPT-3
[00:32:17.720 --> 00:32:19.880]   in production seriously was work.
[00:32:19.880 --> 00:32:21.920]   And most people didn't believe it was the future.
[00:32:21.920 --> 00:32:23.800]   It was like, it's difficult to believe this
[00:32:23.800 --> 00:32:25.280]   because it's only been a year.
[00:32:25.280 --> 00:32:28.120]   And I think everyone has kind of rewritten history.
[00:32:28.120 --> 00:32:30.200]   But I can tell you, because I was trying to do it,
[00:32:30.200 --> 00:32:32.320]   that a year ago, it was still contrarian to say
[00:32:32.320 --> 00:32:33.820]   that large language models were going
[00:32:33.820 --> 00:32:36.160]   to be the default way that people were building things.
[00:32:36.160 --> 00:32:36.800]   Yeah.
[00:32:36.800 --> 00:32:40.160]   Well done for being early on it and convicted enough
[00:32:40.160 --> 00:32:42.000]   to build a leading company doing that.
[00:32:42.000 --> 00:32:43.920]   I think that's commendable.
[00:32:43.920 --> 00:32:46.400]   And I wish I was earlier.
[00:32:46.400 --> 00:32:47.680]   You've still been pretty early.
[00:32:47.680 --> 00:32:48.760]   You've done all right.
[00:32:48.760 --> 00:32:50.440]   I do think-- I do have this message
[00:32:50.440 --> 00:32:52.960]   because I talk to a lot of people who
[00:32:52.960 --> 00:32:54.440]   feel like they've missed it.
[00:32:54.440 --> 00:32:55.760]   But it's just beginning.
[00:32:55.760 --> 00:32:56.840]   It's still so early.
[00:32:56.840 --> 00:32:57.340]   Yeah.
[00:32:57.340 --> 00:33:00.000]   What would you point to to encourage people who feel
[00:33:00.000 --> 00:33:02.760]   like they've missed the boom?
[00:33:02.760 --> 00:33:04.920]   I just think that--
[00:33:04.920 --> 00:33:07.080]   I guess a question to ask yourself if you missed
[00:33:07.080 --> 00:33:10.000]   ChatGPT was, why did you miss it?
[00:33:10.000 --> 00:33:11.680]   And the people who didn't miss it--
[00:33:11.680 --> 00:33:13.100]   and I'm not necessarily including us in this.
[00:33:13.100 --> 00:33:14.640]   I think we were relatively late, even
[00:33:14.640 --> 00:33:16.320]   though we were earlier than most.
[00:33:16.320 --> 00:33:19.080]   What did the people who get it right really grok,
[00:33:19.080 --> 00:33:20.200]   or what do they believe?
[00:33:20.200 --> 00:33:22.880]   What did Ilya Saskovor or Shane Legg,
[00:33:22.880 --> 00:33:25.440]   the people who saw this early--
[00:33:25.440 --> 00:33:29.560]   and I think it was a conviction about deep learning and scale
[00:33:29.560 --> 00:33:33.440]   and projecting forwards that, OK, if we just project
[00:33:33.440 --> 00:33:36.120]   forwards the current improvements from deep learning
[00:33:36.120 --> 00:33:39.560]   and assume they continue, what will the world look like?
[00:33:39.560 --> 00:33:41.760]   And if you do that today--
[00:33:41.760 --> 00:33:43.220]   and obviously, it's extrapolating.
[00:33:43.220 --> 00:33:44.840]   That's not a theory-based prediction.
[00:33:44.840 --> 00:33:45.960]   It's just an extrapolation.
[00:33:45.960 --> 00:33:48.420]   But the extrapolation has been right for a really long time,
[00:33:48.420 --> 00:33:49.960]   so we should take it seriously.
[00:33:49.960 --> 00:33:52.720]   If you extrapolate that forwards just a year or two,
[00:33:52.720 --> 00:33:54.920]   then you find that you would expect the models to be
[00:33:54.920 --> 00:33:56.660]   phenomenally better than they are today.
[00:33:56.660 --> 00:33:58.160]   And they're already at a scale where
[00:33:58.160 --> 00:33:59.820]   you expect large economic disruption,
[00:33:59.820 --> 00:34:02.040]   even if GPT-4 doesn't get better.
[00:34:02.040 --> 00:34:05.600]   And if all we get is GPT vision plus the current model,
[00:34:05.600 --> 00:34:07.880]   we know that there's loads of useful applications
[00:34:07.880 --> 00:34:10.200]   to be built. People are doing it right now.
[00:34:10.200 --> 00:34:11.840]   But they're going to get better.
[00:34:11.840 --> 00:34:14.000]   This is the worst they're ever going to be.
[00:34:14.000 --> 00:34:17.360]   So if this is what's possible today,
[00:34:17.360 --> 00:34:19.060]   I think the hardest challenge, actually,
[00:34:19.060 --> 00:34:22.120]   is to take seriously the fact that in the not-too-distant
[00:34:22.120 --> 00:34:24.640]   future, you will have models even more capable than the ones
[00:34:24.640 --> 00:34:25.680]   we have now.
[00:34:25.680 --> 00:34:27.000]   How do you build for that world?
[00:34:27.000 --> 00:34:28.160]   I think it's a difficult thing to do,
[00:34:28.160 --> 00:34:29.880]   but it's certainly extremely early.
[00:34:29.880 --> 00:34:33.360]   Yeah, I think the quote that resonated with me this past week
[00:34:33.360 --> 00:34:36.320]   was Nat Friedman saying, imagine everything
[00:34:36.320 --> 00:34:39.560]   that we have now with six orders of magnitude more compute.
[00:34:39.560 --> 00:34:41.920]   By the end of the decade, and plan for that.
[00:34:41.920 --> 00:34:44.360]   Yeah, and that seems to me like a--
[00:34:44.360 --> 00:34:45.720]   Six orders is a lot.
[00:34:45.720 --> 00:34:47.560]   Six orders seems optimistic.
[00:34:47.560 --> 00:34:49.320]   But I think it's a good mental exercise,
[00:34:49.320 --> 00:34:50.880]   even if it turned out only to be--
[00:34:50.880 --> 00:34:53.920]   if it was only four orders or only three orders,
[00:34:53.920 --> 00:34:55.800]   it would still be transformative.
[00:34:55.800 --> 00:34:58.440]   If GPT-4, instead of costing $40 million,
[00:34:58.440 --> 00:35:01.160]   or whatever it costs, tens of millions of dollars,
[00:35:01.160 --> 00:35:02.840]   became tens of thousands of dollars.
[00:35:02.840 --> 00:35:05.000]   I've heard a total all-in cost $500 million.
[00:35:05.000 --> 00:35:06.300]   So let's say it was $500 million today,
[00:35:06.300 --> 00:35:08.480]   and it became a million or two million.
[00:35:08.480 --> 00:35:11.320]   That becomes accessible to even startups,
[00:35:11.320 --> 00:35:14.000]   let alone medium-sized companies.
[00:35:14.000 --> 00:35:16.600]   And I think we should assume something like that will happen.
[00:35:16.600 --> 00:35:18.880]   I would say even without significant research
[00:35:18.880 --> 00:35:20.040]   breakthroughs on the modeling side,
[00:35:20.040 --> 00:35:23.000]   I would just expect inference costs to become a lot cheaper.
[00:35:23.000 --> 00:35:24.760]   So training is difficult to optimize
[00:35:24.760 --> 00:35:26.680]   from a research perspective, but figuring out
[00:35:26.680 --> 00:35:29.640]   how to quantize models, how to make hardware more efficient,
[00:35:29.640 --> 00:35:31.720]   that to me feels like you chip away at it,
[00:35:31.720 --> 00:35:33.080]   and it'll just happen naturally.
[00:35:33.080 --> 00:35:34.960]   And we're already seeing signs of that.
[00:35:34.960 --> 00:35:36.760]   So I would expect inference to get phenomenally cheaper,
[00:35:36.760 --> 00:35:37.960]   which is most of the cost.
[00:35:37.960 --> 00:35:41.080]   - Yeah, and a previous guest that we had on
[00:35:41.080 --> 00:35:42.560]   by the time this comes out is Chris Lattner,
[00:35:42.560 --> 00:35:46.040]   who is working on a compilation for Python
[00:35:46.040 --> 00:35:47.720]   that's gonna make inference a lot cheaper,
[00:35:47.720 --> 00:35:49.720]   'cause it's gonna fully saturate the actual compute
[00:35:49.720 --> 00:35:50.560]   that we already have.
[00:35:50.560 --> 00:35:51.980]   - Yeah. (laughs)
[00:35:51.980 --> 00:35:55.040]   So I think it's an easy prediction to make,
[00:35:55.040 --> 00:35:57.840]   that inference costs come down phenomenally.
[00:35:57.840 --> 00:35:59.080]   - Fantastic.
[00:35:59.080 --> 00:36:02.960]   In my mind, you went upmarket faster than most startups
[00:36:02.960 --> 00:36:03.800]   that I talked to.
[00:36:03.800 --> 00:36:05.160]   So you started selling to Enterprise,
[00:36:05.160 --> 00:36:07.840]   and I see you have Duolingo Max and Gusto AI,
[00:36:07.840 --> 00:36:10.080]   as case studies, you have Trust Report.
[00:36:10.080 --> 00:36:11.080]   You don't need SOC 2.
[00:36:11.080 --> 00:36:13.720]   - We're in the process of SOC 2.
[00:36:13.720 --> 00:36:15.600]   So we have SOC 2 Part 1,
[00:36:15.600 --> 00:36:17.880]   and we're currently being audited for SOC 2 Part 2.
[00:36:17.880 --> 00:36:19.680]   - Yeah, but you have the Vanta thing up.
[00:36:19.680 --> 00:36:22.160]   - We have the Vanta thing up, and we have the Part 1,
[00:36:22.160 --> 00:36:24.360]   we have the Trust Report, we have regular pen tests.
[00:36:24.360 --> 00:36:25.640]   We have to do a lot of this stuff
[00:36:25.640 --> 00:36:26.480]   in order to get to the procurement.
[00:36:26.480 --> 00:36:28.040]   - To sell to Enterprise. - Yeah.
[00:36:28.040 --> 00:36:29.600]   - So I mean, I love the Vanta story.
[00:36:29.600 --> 00:36:31.720]   It's not AI, but do you think that
[00:36:31.720 --> 00:36:33.960]   the Vanta Trust Report is gonna work?
[00:36:33.960 --> 00:36:34.800]   - In what sense?
[00:36:34.800 --> 00:36:37.440]   - As a SOC 2 replacement, a SOC 2 proxy.
[00:36:38.440 --> 00:36:40.640]   - I don't know, honestly.
[00:36:40.640 --> 00:36:43.520]   All I can say is that customers still care
[00:36:43.520 --> 00:36:47.400]   that we have SOC 2, and we're still having to go through it.
[00:36:47.400 --> 00:36:49.560]   Even with SOC 2, though, Vanta makes the process
[00:36:49.560 --> 00:36:51.480]   of doing it phenomenally easier.
[00:36:51.480 --> 00:36:52.440]   - Okay, that's a big endorsement.
[00:36:52.440 --> 00:36:53.880]   - So I would endorse the product.
[00:36:53.880 --> 00:36:56.680]   I've been less close to it than my co-founder, Peter,
[00:36:56.680 --> 00:36:57.520]   and a couple of others.
[00:36:57.520 --> 00:36:59.200]   - Oh yeah, there's always a Vanta implementation,
[00:36:59.200 --> 00:37:00.800]   a SOC 2 implementation person,
[00:37:00.800 --> 00:37:01.840]   and that poor person is like,
[00:37:01.840 --> 00:37:03.600]   for a year, they're dealing with this.
[00:37:03.600 --> 00:37:06.560]   - But it's certainly been a lot faster because of that.
[00:37:06.560 --> 00:37:07.720]   But just more broadly,
[00:37:07.720 --> 00:37:10.200]   becoming an enterprise-oriented company,
[00:37:10.200 --> 00:37:12.920]   what have you had to change or learn?
[00:37:12.920 --> 00:37:17.720]   - Yeah, so I would actually say that we've only done it
[00:37:17.720 --> 00:37:20.840]   because we were feeling the pull.
[00:37:20.840 --> 00:37:24.960]   I wouldn't recommend doing it early if you can avoid it,
[00:37:24.960 --> 00:37:26.360]   because you do have to do all these things,
[00:37:26.360 --> 00:37:29.080]   SOC 2 compliance, and I think Peter's filling out
[00:37:29.080 --> 00:37:32.040]   a very long InfoSec questionnaire today, right?
[00:37:32.040 --> 00:37:34.080]   Although you have most of the questions prepared,
[00:37:34.080 --> 00:37:35.600]   each one's just a little bit different,
[00:37:35.600 --> 00:37:36.440]   so there is just this overhead.
[00:37:36.440 --> 00:37:37.360]   - You need GPT to fill it out.
[00:37:37.360 --> 00:37:40.280]   - There is this overhead on each time, no comment.
[00:37:40.280 --> 00:37:43.440]   (both laughing)
[00:37:43.440 --> 00:37:47.120]   But the potential gain for some of these larger companies,
[00:37:47.120 --> 00:37:49.240]   if they can make efficiency improvements
[00:37:49.240 --> 00:37:52.920]   of one, two, four, 5%, is so much bigger,
[00:37:52.920 --> 00:37:54.960]   and the efficiency improvements probably aren't 5%,
[00:37:54.960 --> 00:37:57.040]   they're probably 20%, 30%.
[00:37:57.040 --> 00:37:59.400]   And so when the upside is so large,
[00:37:59.400 --> 00:38:03.640]   if you're a large company that's,
[00:38:03.640 --> 00:38:05.800]   your costs are dominated, say, by customer support
[00:38:05.800 --> 00:38:07.120]   or something like this,
[00:38:07.120 --> 00:38:08.280]   then the idea that you might be able
[00:38:08.280 --> 00:38:10.160]   to dramatically improve that,
[00:38:10.160 --> 00:38:12.360]   or if you can make your developers much more efficient,
[00:38:12.360 --> 00:38:13.800]   there's just no shortage of things.
[00:38:13.800 --> 00:38:15.400]   And I think a lot of companies,
[00:38:15.400 --> 00:38:17.660]   in the build versus buy decision,
[00:38:17.660 --> 00:38:18.720]   they want to do both,
[00:38:18.720 --> 00:38:21.320]   because they want to have the capacity internally
[00:38:21.320 --> 00:38:23.960]   to be able to build AI features and services
[00:38:23.960 --> 00:38:24.960]   as part of their product as well,
[00:38:24.960 --> 00:38:26.240]   so they don't wanna buy everything.
[00:38:26.240 --> 00:38:28.240]   Certain things, it makes a lot of sense,
[00:38:28.240 --> 00:38:30.080]   it's fully packaged, no one's building their own IDE,
[00:38:30.080 --> 00:38:32.720]   like they're gonna use Copilot or whatever is the equivalent
[00:38:32.720 --> 00:38:35.240]   but they want to be able to add,
[00:38:35.240 --> 00:38:37.720]   I think the first AI feature that Gusto added
[00:38:37.720 --> 00:38:40.040]   was the ability within their application
[00:38:40.040 --> 00:38:42.880]   for people who are creating job ads,
[00:38:42.880 --> 00:38:44.600]   could put in a very short description
[00:38:44.600 --> 00:38:47.000]   and it would auto-generate the first draft job ad,
[00:38:47.000 --> 00:38:48.380]   and was smart enough to know
[00:38:48.380 --> 00:38:50.040]   that there are different legal requirements
[00:38:50.040 --> 00:38:52.560]   and what information has to be there for different states.
[00:38:52.560 --> 00:38:54.960]   So in certain states, you have to, for example,
[00:38:54.960 --> 00:38:56.400]   report the salary range,
[00:38:56.400 --> 00:38:57.720]   and in certain states you don't,
[00:38:57.720 --> 00:39:00.960]   it's pretty easy to give that information to GPT-4
[00:39:00.960 --> 00:39:03.480]   and have it generate you a sensible draft.
[00:39:03.480 --> 00:39:06.800]   But that was, I think, something that they got to production
[00:39:06.800 --> 00:39:08.280]   within weeks of starting.
[00:39:08.280 --> 00:39:10.720]   And just to see such a large company
[00:39:10.720 --> 00:39:13.040]   go from zero to having AI features in production,
[00:39:13.040 --> 00:39:15.420]   and now they're adding more and more,
[00:39:15.420 --> 00:39:16.480]   it's been quite phenomenal.
[00:39:16.480 --> 00:39:20.480]   - Yeah, the speed of iteration, it's unlike enterprise,
[00:39:20.480 --> 00:39:21.320]   which is fantastic.
[00:39:21.320 --> 00:39:23.720]   I think a lot of people see the potential there.
[00:39:23.720 --> 00:39:27.040]   I think people's main concern
[00:39:27.040 --> 00:39:29.680]   with having someone like HumanLoop in the loop
[00:39:29.680 --> 00:39:32.280]   is the data and privacy element, right?
[00:39:32.280 --> 00:39:34.240]   Do people want on-prem HumanLoop?
[00:39:34.240 --> 00:39:36.720]   - So we do do VPC deployments where they're needed.
[00:39:36.720 --> 00:39:38.520]   We don't do full on-premise.
[00:39:38.520 --> 00:39:40.840]   So far, most people, we've been able to persuade
[00:39:40.840 --> 00:39:41.800]   that they don't need it.
[00:39:41.800 --> 00:39:44.240]   So whenever someone says we need VPC,
[00:39:44.240 --> 00:39:46.360]   the first question I always ask is why.
[00:39:46.360 --> 00:39:48.800]   And then we go through what are the real reasons,
[00:39:48.800 --> 00:39:50.040]   like what are they concerned about,
[00:39:50.040 --> 00:39:51.460]   and we see whether we can find ways,
[00:39:51.460 --> 00:39:54.440]   either contractually or in our own cloud,
[00:39:54.440 --> 00:39:56.180]   to satisfy those requirements.
[00:39:56.180 --> 00:39:57.740]   There are exceptions, like we work now
[00:39:57.740 --> 00:40:00.440]   with some financially regulated companies.
[00:40:00.440 --> 00:40:02.560]   Amex GPT is one of our customers.
[00:40:02.560 --> 00:40:04.680]   Sorry, I should specify. - I heard GPT, okay.
[00:40:04.680 --> 00:40:07.920]   - Yeah, no, Amex GPT is their global business travel arm.
[00:40:07.920 --> 00:40:10.200]   And they've got very sensitive information.
[00:40:10.200 --> 00:40:12.080]   And so they're particularly concerned about it,
[00:40:12.080 --> 00:40:13.660]   and there's more auditing.
[00:40:13.660 --> 00:40:17.800]   But for the people who are not financially regulated,
[00:40:17.800 --> 00:40:19.840]   usually we can persuade them that,
[00:40:19.840 --> 00:40:22.440]   look, we have SOC 2, or we're essentially there.
[00:40:22.440 --> 00:40:23.960]   We've got regular pen tests.
[00:40:23.960 --> 00:40:26.920]   We follow really high security standards.
[00:40:26.920 --> 00:40:29.360]   Most people so far have been accepting of that.
[00:40:29.360 --> 00:40:31.800]   - Yeah, have you ever attempted to classify
[00:40:31.800 --> 00:40:33.920]   the use cases that you're seeing?
[00:40:33.920 --> 00:40:35.440]   Just, you see the whole universe,
[00:40:35.440 --> 00:40:37.240]   and you're not super opinionated about them,
[00:40:37.240 --> 00:40:41.240]   but there's summarization, there's classification,
[00:40:41.240 --> 00:40:42.640]   there's, you know, there's--
[00:40:42.640 --> 00:40:43.480]   - Okay, so interesting.
[00:40:43.480 --> 00:40:45.800]   I've certainly not tried to classify them
[00:40:45.800 --> 00:40:48.500]   at that granularity, like is it summarization
[00:40:48.500 --> 00:40:49.840]   or a question answering?
[00:40:49.840 --> 00:40:52.540]   I often think more about the end use case.
[00:40:52.540 --> 00:40:55.040]   So is this an ed tech use case, or is this someone--
[00:40:55.040 --> 00:40:55.880]   - That's the vertical to me.
[00:40:55.880 --> 00:40:57.640]   - Yeah. - I think a little bit more
[00:40:57.640 --> 00:40:59.480]   about it like that.
[00:40:59.480 --> 00:41:03.280]   In terms of use cases, it's really varied, right?
[00:41:03.280 --> 00:41:04.920]   There are people using the models as completion,
[00:41:04.920 --> 00:41:06.280]   there's chat.
[00:41:06.280 --> 00:41:08.580]   Like, it wouldn't be so obvious to know
[00:41:08.580 --> 00:41:11.320]   without doing some like GPT level analysis on it,
[00:41:11.320 --> 00:41:14.080]   like getting GPT to look at the outputs and inputs.
[00:41:14.080 --> 00:41:14.920]   - Which you can do.
[00:41:14.920 --> 00:41:18.720]   - Which we can do, whether they are doing summarization
[00:41:18.720 --> 00:41:19.720]   or something similar.
[00:41:19.720 --> 00:41:21.880]   But I would say I feel like most use cases blend.
[00:41:21.880 --> 00:41:23.880]   Like that to me feels like an old school NLP way
[00:41:23.880 --> 00:41:24.840]   of viewing the world.
[00:41:24.840 --> 00:41:27.400]   Like an old school NLP, we used to break down these tasks
[00:41:27.400 --> 00:41:30.640]   into like summarization and NER and extraction and QA
[00:41:30.640 --> 00:41:32.480]   and then pipeline things together.
[00:41:32.480 --> 00:41:35.120]   And actually I feel like that doesn't map very well
[00:41:35.120 --> 00:41:36.680]   onto how people are using GPT for today.
[00:41:36.680 --> 00:41:38.440]   - Stack more layers, bro.
[00:41:38.440 --> 00:41:40.960]   - Because they're using them as general purpose models.
[00:41:40.960 --> 00:41:43.080]   And so it is one model that's doing NER
[00:41:43.080 --> 00:41:44.840]   and it's doing extraction, it's doing summarization,
[00:41:44.840 --> 00:41:46.280]   it's doing classification, and it's often
[00:41:46.280 --> 00:41:49.560]   in one end to end sort of system.
[00:41:49.560 --> 00:41:52.800]   - I think that's what people want to believe,
[00:41:52.800 --> 00:41:54.920]   that they're using them as general purpose models.
[00:41:54.920 --> 00:41:57.520]   But actually when you open up the covers
[00:41:57.520 --> 00:42:00.960]   and look at the volume, 80% of it is some really dumb
[00:42:00.960 --> 00:42:01.800]   use case that you could--
[00:42:01.800 --> 00:42:02.960]   - Like question answering over documents
[00:42:02.960 --> 00:42:03.800]   or something like that.
[00:42:03.800 --> 00:42:04.640]   - Yeah.
[00:42:04.640 --> 00:42:05.460]   - Yeah, that's not--
[00:42:05.460 --> 00:42:07.040]   - I'm trying to get some insight from there, I don't--
[00:42:07.040 --> 00:42:09.800]   - Yeah, so I can tell you the trajectory we've seen.
[00:42:09.800 --> 00:42:13.400]   So really early on, the killer use case was some form
[00:42:13.400 --> 00:42:15.440]   of writing assistant, whether it was a marketing
[00:42:15.440 --> 00:42:16.280]   writing assistant--
[00:42:16.280 --> 00:42:17.120]   - The Jaspers.
[00:42:17.120 --> 00:42:20.080]   - Right, Jasper, Copy AI, we had like seven of them
[00:42:20.080 --> 00:42:20.920]   at one time, right?
[00:42:20.920 --> 00:42:23.120]   And then you had like specialist writing assistants.
[00:42:23.120 --> 00:42:25.320]   Some I think have gone on to be really successful products
[00:42:25.320 --> 00:42:27.360]   like Pseudo-Write or Type AI is another one,
[00:42:27.360 --> 00:42:29.200]   but they're still fundamentally like helping people
[00:42:29.200 --> 00:42:30.400]   write better.
[00:42:30.400 --> 00:42:32.360]   And then I think increasingly we've seen
[00:42:32.360 --> 00:42:34.040]   more diversification.
[00:42:34.040 --> 00:42:35.880]   There was a wave of chat to documents
[00:42:35.880 --> 00:42:37.360]   in one form or another, right?
[00:42:37.360 --> 00:42:38.840]   - Yeah, chat PDF still doing really well.
[00:42:38.840 --> 00:42:39.780]   - Yeah, chat PDF doing super well.
[00:42:39.780 --> 00:42:41.720]   Once RAG started working, like Retrieval Augmented
[00:42:41.720 --> 00:42:43.220]   Generation, there was that.
[00:42:43.220 --> 00:42:46.880]   But since then, as people are more problem driven
[00:42:46.880 --> 00:42:49.200]   and they're like trying to say, okay, how can we use this?
[00:42:49.200 --> 00:42:50.880]   We see a much broader range.
[00:42:50.880 --> 00:42:54.080]   So even within, like take Duolingo as an example,
[00:42:54.080 --> 00:42:55.560]   they've got Duolingo Max.
[00:42:55.560 --> 00:42:57.280]   So that's like a conversational experience,
[00:42:57.280 --> 00:42:59.040]   but they're also using large language models
[00:42:59.040 --> 00:43:00.200]   within the evaluation of that.
[00:43:00.200 --> 00:43:02.080]   They're also using it for content creation.
[00:43:02.080 --> 00:43:03.160]   And each of these companies sort of,
[00:43:03.160 --> 00:43:05.440]   you start with one use case and I feel like it expands
[00:43:05.440 --> 00:43:07.120]   because you just discover more and more things
[00:43:07.120 --> 00:43:09.240]   you can do with the models.
[00:43:09.240 --> 00:43:10.700]   - Yeah, yeah, yeah.
[00:43:10.700 --> 00:43:12.600]   Do you see much code generation?
[00:43:12.600 --> 00:43:13.440]   - Yes.
[00:43:13.440 --> 00:43:17.320]   So I would say that like developer focused tools,
[00:43:17.320 --> 00:43:20.720]   I would say like EdTech and developer focused tools
[00:43:20.720 --> 00:43:22.520]   are like probably two of the biggest areas
[00:43:22.520 --> 00:43:24.200]   that we see people working on.
[00:43:24.200 --> 00:43:25.040]   - Yeah.
[00:43:25.040 --> 00:43:27.160]   I'm always wondering, because code generation
[00:43:27.160 --> 00:43:29.760]   is so structured that you might have some special
[00:43:29.760 --> 00:43:30.640]   affordances for that.
[00:43:30.640 --> 00:43:33.520]   But again, that's entitled a bitter lesson.
[00:43:33.520 --> 00:43:36.120]   I always wonder what we can optimize for,
[00:43:36.120 --> 00:43:38.160]   but that's my optimization brain when I should not.
[00:43:38.160 --> 00:43:39.660]   I should just scale things up.
[00:43:39.660 --> 00:43:43.940]   - I think there's merit in both.
[00:43:43.940 --> 00:43:44.880]   - Yes.
[00:43:44.880 --> 00:43:47.520]   Okay, so today, by the time we release this,
[00:43:47.520 --> 00:43:49.160]   you will have announced your new pricing.
[00:43:49.160 --> 00:43:50.420]   - Yeah, that's right.
[00:43:50.420 --> 00:43:53.120]   So one thing that people have said to us a lot actually
[00:43:53.120 --> 00:43:54.860]   is that the barrier of entry to getting started
[00:43:54.860 --> 00:43:56.480]   with HumanLoop is just quite high.
[00:43:56.480 --> 00:43:59.240]   There isn't, you can't just install an open source package
[00:43:59.240 --> 00:44:01.200]   and just get going or whatever it might be.
[00:44:01.200 --> 00:44:03.460]   And there've been quite a few small companies
[00:44:03.460 --> 00:44:05.280]   that have signed up and then sent us messages,
[00:44:05.280 --> 00:44:07.480]   we're a not-for-profit or an early stage company,
[00:44:07.480 --> 00:44:09.000]   we really want to use HumanLoop,
[00:44:09.000 --> 00:44:10.680]   but it's just prohibitively expensive for now.
[00:44:10.680 --> 00:44:12.480]   We wouldn't mind paying in the future.
[00:44:12.480 --> 00:44:14.700]   And so we've thought really hard about how can we make it,
[00:44:14.700 --> 00:44:17.000]   like lower the barriers to entry for people to try it out
[00:44:17.000 --> 00:44:18.680]   and get started and get value
[00:44:18.680 --> 00:44:21.180]   and have the amount they have to pay scale much more
[00:44:21.180 --> 00:44:22.400]   with the value they get.
[00:44:22.400 --> 00:44:23.740]   So that they're only paying for things
[00:44:23.740 --> 00:44:25.680]   when they've got value from HumanLoop.
[00:44:25.680 --> 00:44:27.460]   And so we will be launching a new set of pricing,
[00:44:27.460 --> 00:44:29.300]   there'll be a free tier, so you can sign up,
[00:44:29.300 --> 00:44:30.740]   you can get going on the website,
[00:44:30.740 --> 00:44:31.880]   you can start building projects
[00:44:31.880 --> 00:44:33.420]   and you won't have to pay anything.
[00:44:33.420 --> 00:44:35.980]   And only once you get to a certain scale,
[00:44:35.980 --> 00:44:37.940]   you've got more than three people on the platform,
[00:44:37.940 --> 00:44:40.060]   you're logging a certain amount of data to us,
[00:44:40.060 --> 00:44:42.780]   then pricing kicks in and it scales with you.
[00:44:42.780 --> 00:44:44.620]   So as your volumes go up,
[00:44:44.620 --> 00:44:46.460]   that's the time when you'll start paying us more.
[00:44:46.460 --> 00:44:47.980]   So much more gradual than it is now.
[00:44:47.980 --> 00:44:50.880]   - And you're tying some features to the tiers.
[00:44:50.880 --> 00:44:53.540]   - A little bit, but mostly we're trying to give you
[00:44:53.540 --> 00:44:56.240]   just a sort of, most of the product experience.
[00:44:56.240 --> 00:44:58.720]   On the free tier, I think there's one or two things
[00:44:58.720 --> 00:45:00.960]   you don't have, but you have almost everything.
[00:45:00.960 --> 00:45:03.280]   And then once you're off the free tier, you have everything,
[00:45:03.280 --> 00:45:05.360]   but you pay, the amount you pay
[00:45:05.360 --> 00:45:06.800]   kind of scales slightly differently.
[00:45:06.800 --> 00:45:08.520]   So you get volume discounts at scale.
[00:45:08.520 --> 00:45:09.360]   - Awesome.
[00:45:09.360 --> 00:45:11.520]   And so this is where one of the hard questions is, right?
[00:45:11.520 --> 00:45:13.120]   Like, is there a graduation risk
[00:45:13.120 --> 00:45:15.440]   as people get very serious about logging?
[00:45:15.440 --> 00:45:17.080]   You brought up Datadog earlier
[00:45:17.080 --> 00:45:19.860]   and for sure Datadog is looking at your market
[00:45:19.860 --> 00:45:22.120]   as much as you're looking at theirs.
[00:45:22.120 --> 00:45:24.580]   So how do you think about that of like,
[00:45:24.580 --> 00:45:27.980]   ultimately at scale becomes a commodity, right?
[00:45:27.980 --> 00:45:28.920]   The logging.
[00:45:28.920 --> 00:45:32.200]   - So I think that actually this is really different to that.
[00:45:32.200 --> 00:45:33.840]   So the more people use it,
[00:45:33.840 --> 00:45:35.360]   we find actually the stickier it becomes.
[00:45:35.360 --> 00:45:36.240]   It's almost the opposite.
[00:45:36.240 --> 00:45:37.600]   That as they get to scale,
[00:45:37.600 --> 00:45:40.400]   so you're right that the millionth feedback data point
[00:45:40.400 --> 00:45:43.440]   is worth a lot less than the thousandth feedback data point.
[00:45:43.440 --> 00:45:45.400]   But what continues to be really valuable
[00:45:45.400 --> 00:45:48.160]   is this infrastructure around the workflow
[00:45:48.160 --> 00:45:50.400]   of prompt management, engineering, fixing things.
[00:45:50.400 --> 00:45:51.800]   So we see, you know, you have,
[00:45:51.800 --> 00:45:53.720]   what happens over time is people put more
[00:45:53.720 --> 00:45:55.560]   and more evaluations onto Human Loop.
[00:45:55.560 --> 00:45:57.000]   They've got more people on their team,
[00:45:57.000 --> 00:45:59.160]   the product manager and also three linguists
[00:45:59.160 --> 00:46:01.560]   and someone else who are opening up the data
[00:46:01.560 --> 00:46:02.880]   that's being logged through Human Loop
[00:46:02.880 --> 00:46:04.720]   back into that interactive environment.
[00:46:04.720 --> 00:46:05.820]   They're rerunning things,
[00:46:05.820 --> 00:46:07.760]   they're plugging in other data sources.
[00:46:07.760 --> 00:46:10.640]   And so over time, actually, the raw logs,
[00:46:10.640 --> 00:46:12.680]   I agree with you, kind of become commoditized.
[00:46:12.680 --> 00:46:15.960]   But the tooling that's needed to be able to
[00:46:15.960 --> 00:46:17.560]   not just kind of collect the data,
[00:46:17.560 --> 00:46:19.880]   but make it useful and do something with it
[00:46:19.880 --> 00:46:21.120]   to improve your model,
[00:46:21.120 --> 00:46:23.000]   that's the bit that becomes more valuable, right?
[00:46:23.000 --> 00:46:24.720]   Once you have something working at scale,
[00:46:24.720 --> 00:46:26.700]   then improving it by a few percentage points
[00:46:26.700 --> 00:46:28.480]   is like very, very impactful.
[00:46:28.480 --> 00:46:31.400]   So a lot of our customers early on
[00:46:31.400 --> 00:46:32.440]   would say exactly this to us,
[00:46:32.440 --> 00:46:34.240]   like, "Oh, we can just dump our logs
[00:46:34.240 --> 00:46:36.280]   to like an S3 bucket or we can plug it."
[00:46:36.280 --> 00:46:38.880]   And then like, "Why do we need a special purpose tool?"
[00:46:38.880 --> 00:46:40.760]   And most of them come back to us later
[00:46:40.760 --> 00:46:41.680]   because what they find is,
[00:46:41.680 --> 00:46:43.440]   "Oh, okay, I've logged something,
[00:46:43.440 --> 00:46:46.040]   but it's really difficult for me to like match up the log
[00:46:46.040 --> 00:46:47.840]   to like what model generated it,
[00:46:47.840 --> 00:46:50.000]   and then quickly run that and try something else."
[00:46:50.000 --> 00:46:51.320]   Or I've like logged something
[00:46:51.320 --> 00:46:53.600]   and that log involved a retrieval.
[00:46:53.600 --> 00:46:55.760]   And I would like to know like what went wrong with retrieval
[00:46:55.760 --> 00:46:58.160]   or which document the retrieval came from,
[00:46:58.160 --> 00:47:00.440]   and I didn't log that information correctly,
[00:47:00.440 --> 00:47:01.280]   et cetera, et cetera.
[00:47:01.280 --> 00:47:04.000]   And like the complexity of like setting this up well
[00:47:04.000 --> 00:47:04.820]   is quite high.
[00:47:04.820 --> 00:47:06.340]   So you can either spend a lot of time,
[00:47:06.340 --> 00:47:09.040]   at that stage, two things happen.
[00:47:09.040 --> 00:47:11.200]   Either people roll their own solution,
[00:47:11.200 --> 00:47:12.640]   and early on, we saw a lot of people
[00:47:12.640 --> 00:47:14.040]   build their own solutions,
[00:47:14.040 --> 00:47:15.540]   or they come and use something like us.
[00:47:15.540 --> 00:47:16.600]   And I think increasingly,
[00:47:16.600 --> 00:47:19.280]   because we've been working on this for now more than a year,
[00:47:19.280 --> 00:47:21.800]   the difference between something you would build yourself
[00:47:21.800 --> 00:47:25.200]   and sort of a bot solution is now quite enormous.
[00:47:25.200 --> 00:47:26.820]   And so I just wouldn't recommend it.
[00:47:26.820 --> 00:47:28.840]   And I guess the difference on the Datadog point
[00:47:28.840 --> 00:47:30.160]   or like other analytics tools,
[00:47:30.160 --> 00:47:31.980]   you mentioned Amplitude or Datadog,
[00:47:31.980 --> 00:47:34.000]   they're much more about passive monitoring.
[00:47:34.000 --> 00:47:36.700]   And I think one of the amazing things about AI
[00:47:36.700 --> 00:47:38.120]   is the interventions you can take
[00:47:38.120 --> 00:47:39.840]   can be very quick and very powerful.
[00:47:39.840 --> 00:47:41.240]   And so coupling very closely
[00:47:41.240 --> 00:47:43.180]   the ability to update a retrieval system
[00:47:43.180 --> 00:47:45.920]   or change a prompt to the analytics data
[00:47:45.920 --> 00:47:47.280]   and allowing you to run those experiments,
[00:47:47.280 --> 00:47:48.600]   I think is very powerful.
[00:47:48.600 --> 00:47:49.440]   - Fantastic answer.
[00:47:49.440 --> 00:47:50.960]   It's almost like we prepped for this.
[00:47:50.960 --> 00:47:51.840]   (both laughing)
[00:47:51.840 --> 00:47:53.760]   - It's also almost like I think about this a lot.
[00:47:53.760 --> 00:47:55.160]   And if I didn't have an answer to that question,
[00:47:55.160 --> 00:47:56.280]   it would be difficult to justify
[00:47:56.280 --> 00:47:57.700]   spending all my time building this.
[00:47:57.700 --> 00:47:59.420]   But I do think it's very important.
[00:47:59.420 --> 00:48:00.260]   - Yeah.
[00:48:00.260 --> 00:48:01.080]   Company building,
[00:48:01.080 --> 00:48:03.600]   what have you changed your mind on as a founder?
[00:48:03.600 --> 00:48:05.120]   - Ah, that's a great question.
[00:48:05.120 --> 00:48:06.200]   So one thing that comes to my mind
[00:48:06.200 --> 00:48:07.440]   as soon as you say company building
[00:48:07.440 --> 00:48:09.800]   is like a piece of advice that Michael Siebel has at YC
[00:48:09.800 --> 00:48:11.680]   which is like, don't do it,
[00:48:11.680 --> 00:48:13.680]   or at least don't do it pre-PMF.
[00:48:13.680 --> 00:48:16.680]   Like one of the biggest failure modes
[00:48:16.680 --> 00:48:18.520]   of early stage startups is,
[00:48:18.520 --> 00:48:19.800]   especially if they've raised investment
[00:48:19.800 --> 00:48:22.520]   from large investors,
[00:48:22.520 --> 00:48:24.480]   is that they persuade themselves
[00:48:24.480 --> 00:48:25.440]   that they have PMF too early
[00:48:25.440 --> 00:48:27.720]   and they go into sort of scaling mode and hiring people.
[00:48:27.720 --> 00:48:29.920]   And a lot of that stuff is important
[00:48:29.920 --> 00:48:32.040]   but distracts from the most important thing
[00:48:32.040 --> 00:48:33.140]   that you have to do,
[00:48:33.140 --> 00:48:34.600]   which is understand the needs
[00:48:34.600 --> 00:48:36.120]   that are most pressing for your customer,
[00:48:36.120 --> 00:48:37.880]   figure out who the right customer is
[00:48:37.880 --> 00:48:39.720]   and build what they really want.
[00:48:39.720 --> 00:48:41.320]   Or if they don't necessarily know what they want,
[00:48:41.320 --> 00:48:42.720]   build what they really need.
[00:48:42.720 --> 00:48:46.680]   So one thing that I believed and I still believe
[00:48:46.680 --> 00:48:48.280]   is that you want to do that at the right time.
[00:48:48.280 --> 00:48:50.920]   That company building too early is a distraction.
[00:48:50.920 --> 00:48:52.280]   - And when was that for you?
[00:48:52.280 --> 00:48:56.120]   - So for us, it was actually November, December last year.
[00:48:56.120 --> 00:48:57.780]   So November, December, 2020.
[00:48:57.780 --> 00:49:00.600]   So we were a four person company for almost two years.
[00:49:00.600 --> 00:49:02.400]   And it was only when everything was breaking,
[00:49:02.400 --> 00:49:04.040]   when all the charts were up and to the right
[00:49:04.040 --> 00:49:07.120]   and we really could not service our customers anymore
[00:49:07.120 --> 00:49:08.620]   because the team was too small.
[00:49:08.620 --> 00:49:11.000]   That's when we started actively hiring people.
[00:49:11.000 --> 00:49:13.200]   And even then we've been really slow
[00:49:13.200 --> 00:49:14.400]   and deliberate about it.
[00:49:14.400 --> 00:49:15.480]   Maybe a little bit too slow,
[00:49:15.480 --> 00:49:17.960]   given how much, there was a lot of suffering
[00:49:17.960 --> 00:49:19.120]   in being that slow.
[00:49:19.120 --> 00:49:21.280]   I wish we had a couple more people when things took off.
[00:49:21.280 --> 00:49:22.200]   There was a period of time,
[00:49:22.200 --> 00:49:24.840]   I would say from November to March,
[00:49:24.840 --> 00:49:27.520]   where all of us were barely functioning
[00:49:27.520 --> 00:49:29.320]   because there was just so much to do.
[00:49:29.320 --> 00:49:34.660]   But we've continued to have the bar set really, really high
[00:49:34.660 --> 00:49:37.040]   and higher slowly and very deliberately.
[00:49:37.040 --> 00:49:40.020]   And I think we get more done with a smaller team
[00:49:40.020 --> 00:49:43.100]   of really, really excellent people
[00:49:43.100 --> 00:49:45.320]   than we would had we hired more people sooner.
[00:49:45.320 --> 00:49:46.980]   So that's something I kind of agreed on.
[00:49:46.980 --> 00:49:49.500]   The other thing that has maybe changed a little bit
[00:49:49.500 --> 00:49:52.540]   in my mind is related to how opinionated you should be.
[00:49:52.540 --> 00:49:54.300]   So I think you asked this question
[00:49:54.300 --> 00:49:56.780]   about opinionation in the product.
[00:49:56.780 --> 00:50:00.500]   And I think there's a risk of just listening
[00:50:00.500 --> 00:50:02.700]   to your customers and building what they want
[00:50:02.700 --> 00:50:04.460]   that can lead to hill climbing.
[00:50:04.460 --> 00:50:06.320]   And I think especially,
[00:50:06.320 --> 00:50:07.580]   and we were guilty of this, I think,
[00:50:07.580 --> 00:50:10.260]   a little bit early on in the first year of Human Lube.
[00:50:10.260 --> 00:50:11.900]   - Oh, you did it well, better than most.
[00:50:11.900 --> 00:50:12.800]   - Thank you.
[00:50:12.800 --> 00:50:15.400]   But I think that where things started working for us
[00:50:15.400 --> 00:50:18.220]   was when we had a lot more strength in our convictions.
[00:50:18.220 --> 00:50:20.220]   When we said, actually, we believe GPT-3
[00:50:20.220 --> 00:50:22.940]   is gonna be the future of how people build this.
[00:50:22.940 --> 00:50:24.260]   And even if people don't believe that today,
[00:50:24.260 --> 00:50:25.900]   we're gonna build for that future.
[00:50:25.900 --> 00:50:26.940]   That is hard to do.
[00:50:26.940 --> 00:50:28.900]   I still think we don't do it enough.
[00:50:28.900 --> 00:50:30.540]   Like I want us to do it even more.
[00:50:30.540 --> 00:50:32.900]   We have things we believe about the future
[00:50:32.900 --> 00:50:34.700]   that are somewhat contrarian.
[00:50:34.700 --> 00:50:37.020]   And being able to plan for that and be opinionated
[00:50:37.020 --> 00:50:38.780]   and build for that future.
[00:50:38.780 --> 00:50:40.460]   And also to be building the things
[00:50:40.460 --> 00:50:41.860]   that we believe our customers need,
[00:50:41.860 --> 00:50:43.680]   not exactly what they ask for.
[00:50:43.680 --> 00:50:45.820]   Because otherwise you end up, I think,
[00:50:45.820 --> 00:50:48.540]   with a lot of very undifferentiated products
[00:50:48.540 --> 00:50:50.740]   that are there for everybody, so they're not for anyone.
[00:50:50.740 --> 00:50:53.340]   And they don't have a strong point of view.
[00:50:53.340 --> 00:50:54.940]   So I think, especially for building dev tools,
[00:50:54.940 --> 00:50:56.140]   I think you should have a point of view.
[00:50:56.140 --> 00:50:57.980]   - Yes, I strongly agree with that.
[00:50:57.980 --> 00:50:59.900]   Hiring, what are you hiring for?
[00:50:59.900 --> 00:51:02.540]   And given that you're now hybrid,
[00:51:02.540 --> 00:51:04.540]   you're spending some time as an NSF,
[00:51:04.540 --> 00:51:05.820]   where are you hiring?
[00:51:05.820 --> 00:51:08.740]   - Yeah, so we're hiring in both SF and London.
[00:51:08.740 --> 00:51:11.800]   The role that is most urgent for me right now, personally,
[00:51:11.800 --> 00:51:14.220]   is hiring for a developer relations engineer.
[00:51:14.220 --> 00:51:17.000]   So this is an engineer who loves community,
[00:51:17.000 --> 00:51:20.580]   loves documentation, loves giving talks, building demos.
[00:51:20.580 --> 00:51:22.060]   As part of launching this new pricing,
[00:51:22.060 --> 00:51:23.320]   where we're gonna have a free tier,
[00:51:23.320 --> 00:51:25.320]   is also having a much bigger push
[00:51:25.320 --> 00:51:28.940]   towards helping individual developers and smaller teams
[00:51:28.940 --> 00:51:30.600]   succeed with HumanLoop as well.
[00:51:30.600 --> 00:51:32.260]   And even developers in larger companies
[00:51:32.260 --> 00:51:35.620]   who just wanna try it out before they're at scale.
[00:51:35.620 --> 00:51:37.180]   And I think to do that well
[00:51:37.180 --> 00:51:40.860]   requires a really good onboarding experience,
[00:51:40.860 --> 00:51:42.580]   really amazing documentation,
[00:51:42.580 --> 00:51:44.220]   and really good community building.
[00:51:44.220 --> 00:51:46.300]   And we need someone fully focused on that.
[00:51:46.300 --> 00:51:47.940]   I don't think it can be someone's part-time job.
[00:51:47.940 --> 00:51:51.320]   We want someone 100% focused on building community.
[00:51:51.320 --> 00:51:53.540]   Ideally, we'd find someone as good as you, Swix,
[00:51:53.540 --> 00:51:54.360]   who to do this job.
[00:51:54.360 --> 00:51:56.180]   So yeah, so if you're a developer relations engineer,
[00:51:56.180 --> 00:51:58.820]   or even if you're just a product-focused engineer
[00:51:58.820 --> 00:52:00.820]   who is excited about AI and ML,
[00:52:00.820 --> 00:52:04.060]   and has some track record of community building,
[00:52:04.060 --> 00:52:06.940]   then that's the role that I would love to hear about.
[00:52:06.940 --> 00:52:09.500]   And we'll be hiring for it primarily in San Francisco.
[00:52:09.500 --> 00:52:11.860]   Although if you are amazing elsewhere, we'll consider it,
[00:52:11.860 --> 00:52:13.580]   but SF being the focus.
[00:52:13.580 --> 00:52:15.340]   - Yeah, thanks for the compliment as well.
[00:52:15.340 --> 00:52:18.300]   But yes, I'd highly recommend people check out the job.
[00:52:18.300 --> 00:52:20.060]   It's already live on the website.
[00:52:20.060 --> 00:52:22.180]   But a lot of people don't know, I have a third blog
[00:52:22.180 --> 00:52:23.700]   that is specifically for dev rel advising,
[00:52:23.700 --> 00:52:25.300]   'cause I do some age-old investing
[00:52:25.300 --> 00:52:27.100]   and people ask me for advice all the time.
[00:52:27.100 --> 00:52:30.540]   And I actually cache my frequently asked questions there.
[00:52:30.540 --> 00:52:31.380]   - Oh, for sure.
[00:52:31.380 --> 00:52:33.580]   - Anything else on the company side that I didn't touch on?
[00:52:33.580 --> 00:52:35.300]   - If you're within YC, this will be boring,
[00:52:35.300 --> 00:52:36.500]   but if you're outside of YC,
[00:52:36.500 --> 00:52:38.860]   I think that you probably can't hear this enough times,
[00:52:38.860 --> 00:52:41.140]   'cause I've seen so many people get this wrong,
[00:52:41.140 --> 00:52:43.780]   which is just like, before PMF,
[00:52:43.780 --> 00:52:45.520]   nothing other than PMF matters.
[00:52:45.520 --> 00:52:49.260]   And there's just so many possible distractions
[00:52:49.260 --> 00:52:51.600]   as a startup founder, or things you could be doing
[00:52:51.600 --> 00:52:53.340]   that sort of feel productive,
[00:52:53.340 --> 00:52:55.220]   but don't actually get you closer to your goal.
[00:52:55.220 --> 00:52:57.380]   Like trying to narrow focus to finding PMF,
[00:52:57.380 --> 00:52:59.500]   and what that means will be a little bit different
[00:52:59.500 --> 00:53:02.060]   for different startups and different experiences.
[00:53:02.060 --> 00:53:03.380]   I have friends who are doing deep tech,
[00:53:03.380 --> 00:53:04.740]   biotech startups or whatever, right?
[00:53:04.740 --> 00:53:07.060]   And so I don't think there's a one size fits all,
[00:53:07.060 --> 00:53:09.740]   but try not to do anything else.
[00:53:09.740 --> 00:53:12.940]   That advice has been really good for us.
[00:53:12.940 --> 00:53:16.340]   And it's often not intuitive.
[00:53:16.340 --> 00:53:19.980]   - Yeah, does HumanLoop have PMF right now?
[00:53:19.980 --> 00:53:22.220]   - I think we have PMF within niches.
[00:53:22.220 --> 00:53:23.820]   So I think we definitely have,
[00:53:23.820 --> 00:53:26.580]   especially for, I would say, if you're a team,
[00:53:26.580 --> 00:53:29.300]   building an LM application within a larger company
[00:53:29.300 --> 00:53:31.300]   then like, yes, we see people sign up,
[00:53:31.300 --> 00:53:34.020]   they use the product, more people use it over time,
[00:53:34.020 --> 00:53:36.260]   usage goes up, they give us great feedback.
[00:53:36.260 --> 00:53:38.700]   There's always room for improvement,
[00:53:38.700 --> 00:53:40.780]   but we have a form of PMF.
[00:53:40.780 --> 00:53:43.620]   And I think there will be like multiple stages of it,
[00:53:43.620 --> 00:53:46.500]   but we certainly found some PMF, yeah.
[00:53:46.500 --> 00:53:49.460]   - What is the next tier of PMF that you're looking for?
[00:53:49.460 --> 00:53:50.980]   - Well, I'm hoping it's on this evals project
[00:53:50.980 --> 00:53:51.820]   that we're launching, right?
[00:53:51.820 --> 00:53:53.660]   So we definitely have PMF on the current
[00:53:53.660 --> 00:53:55.340]   sort of prompt versioning management stuff.
[00:53:55.340 --> 00:53:58.460]   We've got about 10 companies currently in closed beta
[00:53:58.460 --> 00:54:00.500]   on evals, giving us a lot of feedback on it.
[00:54:00.500 --> 00:54:01.980]   It's a real problem for them.
[00:54:01.980 --> 00:54:03.540]   We've seen them get value from it,
[00:54:03.540 --> 00:54:05.220]   but we haven't launched it publicly yet.
[00:54:05.220 --> 00:54:07.260]   I'm hoping that will be the next big one.
[00:54:07.260 --> 00:54:09.300]   - Yeah, just a technical question on the evals,
[00:54:09.300 --> 00:54:11.020]   which I don't know if it's too small,
[00:54:11.020 --> 00:54:13.340]   but typically evals involve writing code.
[00:54:13.340 --> 00:54:14.180]   - Yeah.
[00:54:14.180 --> 00:54:16.500]   - So it's like freeform, Python, JavaScript,
[00:54:16.500 --> 00:54:17.900]   something like that for you guys?
[00:54:17.900 --> 00:54:20.060]   - Yeah, so it's the combination of,
[00:54:20.060 --> 00:54:21.300]   and again, we're iterating on this,
[00:54:21.300 --> 00:54:23.020]   but yeah, you can define them in Python
[00:54:23.020 --> 00:54:25.100]   and they can also call language models as well.
[00:54:25.100 --> 00:54:27.220]   - And it's executed on your servers?
[00:54:27.220 --> 00:54:28.100]   - Both are options.
[00:54:28.100 --> 00:54:28.940]   - Okay, interesting.
[00:54:28.940 --> 00:54:30.500]   - So we have a protected environment.
[00:54:30.500 --> 00:54:32.700]   You can basically execute everything on our servers.
[00:54:32.700 --> 00:54:33.980]   - Well done.
[00:54:33.980 --> 00:54:35.820]   - Which was not easy to build.
[00:54:35.820 --> 00:54:37.260]   I'm not the right person to talk about it,
[00:54:37.260 --> 00:54:39.140]   but I think there's a really interesting engineering blog
[00:54:39.140 --> 00:54:41.220]   and how you can make it safe for other people
[00:54:41.220 --> 00:54:43.080]   to exec code on your servers.
[00:54:43.080 --> 00:54:46.340]   But also it's gonna be set up such that
[00:54:46.340 --> 00:54:47.480]   you can also run things on yours
[00:54:47.480 --> 00:54:48.900]   and just have the output logs
[00:54:48.900 --> 00:54:50.580]   still go to human loop and use for work.
[00:54:50.580 --> 00:54:53.180]   - This is the promise of the edge clouds of the world.
[00:54:53.180 --> 00:54:54.020]   - Yeah.
[00:54:54.020 --> 00:54:56.780]   - The denos, the Cloudflare workers, the models.
[00:54:56.780 --> 00:54:58.000]   I don't know if you've explored any of those,
[00:54:58.000 --> 00:55:00.500]   but then you would not have to set it up yourself.
[00:55:00.500 --> 00:55:02.020]   - I'm pretty sure they've all been explored
[00:55:02.020 --> 00:55:03.260]   by my team in recent months.
[00:55:03.260 --> 00:55:04.580]   - Yeah, yeah.
[00:55:04.580 --> 00:55:05.420]   All right, excellent.
[00:55:05.420 --> 00:55:07.220]   Okay, broughting it out to market takes.
[00:55:07.220 --> 00:55:08.060]   - Yeah.
[00:55:08.060 --> 00:55:08.900]   - Just the, you know,
[00:55:08.900 --> 00:55:10.060]   bringing out for human loop as a whole.
[00:55:10.060 --> 00:55:11.300]   How do you feel about LLM ops
[00:55:11.300 --> 00:55:13.380]   or prompt ops as a category term?
[00:55:13.380 --> 00:55:16.040]   - LLM ops, I would drop one L firstly.
[00:55:16.040 --> 00:55:17.820]   I think we call them large language models today,
[00:55:17.820 --> 00:55:19.900]   but the goalpost of large is gonna keep moving.
[00:55:19.900 --> 00:55:22.860]   So I think the point is sort of foundation models or--
[00:55:22.860 --> 00:55:24.580]   - Oh, I have a proposal to do with that.
[00:55:24.580 --> 00:55:25.400]   - Oh yeah?
[00:55:25.400 --> 00:55:26.240]   - I have t-shirt sizing.
[00:55:26.240 --> 00:55:27.060]   - T-shirt sizing.
[00:55:27.060 --> 00:55:29.660]   - So you're gonna find S, XS, and then M, and L,
[00:55:29.660 --> 00:55:30.500]   all the way to XXL.
[00:55:30.500 --> 00:55:32.800]   - You're just gonna have to keep updating that over time.
[00:55:32.800 --> 00:55:35.560]   But I think foundation model ops is maybe a better term
[00:55:35.560 --> 00:55:38.080]   because I also think that like within six months,
[00:55:38.080 --> 00:55:38.920]   we're gonna have images
[00:55:38.920 --> 00:55:40.380]   and then people won't call them
[00:55:40.380 --> 00:55:41.760]   just language models anymore.
[00:55:41.760 --> 00:55:44.120]   - Yeah, and is it worth a separate category than ML ops?
[00:55:44.120 --> 00:55:46.040]   - But I do think it's worth a separate category.
[00:55:46.040 --> 00:55:46.880]   - Okay.
[00:55:46.880 --> 00:55:48.920]   - I think that the people from it's for are different.
[00:55:48.920 --> 00:55:50.420]   We discussed this a little bit earlier, right?
[00:55:50.420 --> 00:55:51.480]   But a machine learning engineer
[00:55:51.480 --> 00:55:52.600]   and a traditional software engineer
[00:55:52.600 --> 00:55:53.920]   are very different people.
[00:55:53.920 --> 00:55:55.840]   They have different levels of knowledge
[00:55:55.840 --> 00:55:57.320]   and different goals.
[00:55:57.320 --> 00:55:59.880]   I also think that the generality of the models
[00:55:59.880 --> 00:56:01.920]   has changed what people are building.
[00:56:01.920 --> 00:56:03.920]   And so the problems they face are really different.
[00:56:03.920 --> 00:56:05.880]   It's, you know, like what you need
[00:56:05.880 --> 00:56:09.520]   for building a small recommender system at enormous scale
[00:56:09.520 --> 00:56:11.080]   is very different from what you need
[00:56:11.080 --> 00:56:13.680]   to build a generative AI application
[00:56:13.680 --> 00:56:14.720]   that's very subjective.
[00:56:14.720 --> 00:56:15.560]   - Yeah.
[00:56:15.560 --> 00:56:16.720]   - And so I do think that they have,
[00:56:16.720 --> 00:56:18.960]   I actually think like we've seen a lot of ML ops companies
[00:56:18.960 --> 00:56:21.800]   recently try to pivot into solving problems in this space.
[00:56:21.800 --> 00:56:23.320]   And I think it's gonna be hard for them
[00:56:23.320 --> 00:56:25.680]   because they're changing who they're building for.
[00:56:25.680 --> 00:56:28.040]   So they now have to straddle two different
[00:56:28.040 --> 00:56:30.320]   sort of ideal customer profiles.
[00:56:30.320 --> 00:56:32.040]   And they also have a lot of legacy infrastructure
[00:56:32.040 --> 00:56:33.560]   focused around models whose output
[00:56:33.560 --> 00:56:36.000]   was like a measurable quantifiable number.
[00:56:36.000 --> 00:56:38.720]   It was F1 or it was accuracy or something like this.
[00:56:38.720 --> 00:56:41.080]   And I think their lives are gonna keep getting harder
[00:56:41.080 --> 00:56:44.080]   as the models go more general and go multimodal.
[00:56:44.080 --> 00:56:44.920]   - Yeah.
[00:56:44.920 --> 00:56:45.800]   - Because what they've built so far
[00:56:45.800 --> 00:56:47.000]   just won't fit that world.
[00:56:47.000 --> 00:56:47.840]   - Yeah.
[00:56:47.840 --> 00:56:48.920]   - So I think it probably can be done,
[00:56:48.920 --> 00:56:50.300]   but I think it's gonna be very hard.
[00:56:50.300 --> 00:56:52.160]   - You mentioned deep store vision,
[00:56:52.160 --> 00:56:53.720]   and obviously there's more multimodal
[00:56:53.720 --> 00:56:54.640]   models coming along the way.
[00:56:54.640 --> 00:56:56.280]   How big does that factor into your planning?
[00:56:56.280 --> 00:56:59.080]   Because you're very language oriented right now.
[00:56:59.080 --> 00:57:02.040]   - So it's increasingly like an internal conversation.
[00:57:02.040 --> 00:57:03.840]   Every time we have a product roadmap discussion,
[00:57:03.840 --> 00:57:06.460]   like planning for and starting to iterate on
[00:57:06.460 --> 00:57:08.520]   and when to build in support for vision.
[00:57:08.520 --> 00:57:09.360]   - Yeah.
[00:57:09.360 --> 00:57:10.320]   - Has become very much front of mind.
[00:57:10.320 --> 00:57:13.240]   So I think like now, like we're working on it.
[00:57:13.240 --> 00:57:14.080]   - Okay.
[00:57:14.080 --> 00:57:16.400]   One version of this, I pose this exact same question
[00:57:16.400 --> 00:57:19.520]   to Harrison, which is, let's say the GPC4 Vision API
[00:57:19.520 --> 00:57:20.360]   drops tomorrow.
[00:57:20.360 --> 00:57:21.180]   - Yeah.
[00:57:21.180 --> 00:57:22.480]   - What changes in human loop?
[00:57:22.480 --> 00:57:24.360]   - Well, for one thing that you need just to be able,
[00:57:24.360 --> 00:57:25.720]   I mean like very simple things, right?
[00:57:25.720 --> 00:57:28.440]   Like we need to be able to render and read in images
[00:57:28.440 --> 00:57:30.160]   in the playground environment that's interactive, right?
[00:57:30.160 --> 00:57:32.560]   So there's a bunch of just kind of follow your nose things
[00:57:32.560 --> 00:57:34.160]   that I think we'd have to figure out.
[00:57:34.160 --> 00:57:36.360]   But as I said, we've just started working on this.
[00:57:36.360 --> 00:57:38.740]   It's sort of become a product roadmap item.
[00:57:38.740 --> 00:57:41.120]   We, but not like we have to support it.
[00:57:41.120 --> 00:57:42.240]   Like it's very clear.
[00:57:42.240 --> 00:57:44.720]   This is not a question of if, it's a question of when.
[00:57:44.720 --> 00:57:45.560]   - Okay.
[00:57:45.560 --> 00:57:47.240]   Yeah, yeah, excellent.
[00:57:47.240 --> 00:57:48.560]   Is prompt engineering dead?
[00:57:48.560 --> 00:57:50.320]   (laughs)
[00:57:50.320 --> 00:57:52.640]   - So we talked about this a little bit on the walk here,
[00:57:52.640 --> 00:57:54.760]   and I've never been a huge fan of the phrase
[00:57:54.760 --> 00:57:58.400]   prompt engineering, because I think it's simultaneously
[00:57:58.400 --> 00:57:59.920]   makes it not important enough
[00:57:59.920 --> 00:58:01.620]   and too important at the same time.
[00:58:01.620 --> 00:58:02.960]   I don't think it's a form of engineering
[00:58:02.960 --> 00:58:04.600]   in the way that software is a form of engineering,
[00:58:04.600 --> 00:58:06.920]   where it has this rich body of literature and theory
[00:58:06.920 --> 00:58:07.760]   and you have to learn about it
[00:58:07.760 --> 00:58:09.560]   and it takes like very specialist skill.
[00:58:09.560 --> 00:58:11.800]   I think you can get good at it very quickly,
[00:58:11.800 --> 00:58:14.780]   but I do think that prompts are a very important part
[00:58:14.780 --> 00:58:17.480]   of LM or AI applications, right?
[00:58:17.480 --> 00:58:19.640]   Like natural language written instructions
[00:58:19.640 --> 00:58:21.820]   have become part of your source code
[00:58:21.820 --> 00:58:23.460]   and they have impacts on your product quality,
[00:58:23.460 --> 00:58:24.840]   they have impacts on the way your product behaves.
[00:58:24.840 --> 00:58:26.760]   So you should be treated with that level of seriousness
[00:58:26.760 --> 00:58:29.400]   as you would any other code artifact.
[00:58:29.400 --> 00:58:31.080]   So in that sense, I don't think it's dead.
[00:58:31.080 --> 00:58:32.240]   I think it's alive and well
[00:58:32.240 --> 00:58:34.000]   and becoming increasingly important.
[00:58:34.000 --> 00:58:36.600]   It's interesting, there's like, you know,
[00:58:36.600 --> 00:58:40.040]   Anthropic had that very well paid job prompt engineer.
[00:58:40.040 --> 00:58:42.240]   And I think they've hired a few prompt engineers now as well
[00:58:42.240 --> 00:58:44.680]   and those people are leading on deployments in Anthropic
[00:58:44.680 --> 00:58:45.600]   and adding a lot of value.
[00:58:45.600 --> 00:58:48.040]   So there's clearly, it's clearly happening,
[00:58:48.040 --> 00:58:49.840]   but I think maybe it's slightly misnamed.
[00:58:49.840 --> 00:58:52.580]   I actually prefer your kind of AI engineer framing
[00:58:52.580 --> 00:58:55.220]   where this is a different engineering skill set.
[00:58:55.220 --> 00:58:56.380]   You still need to be able to build product,
[00:58:56.380 --> 00:58:57.540]   you're still an engineer,
[00:58:57.540 --> 00:59:00.060]   but you have an intuition for how to get the best
[00:59:00.060 --> 00:59:01.940]   out of models, how to evaluate them.
[00:59:01.940 --> 00:59:05.160]   You understand the problems that come from stochasticity
[00:59:05.160 --> 00:59:07.540]   and you also understand just the nuances.
[00:59:07.540 --> 00:59:08.740]   Like if you have a good mental model
[00:59:08.740 --> 00:59:10.700]   for how a large language model works,
[00:59:10.700 --> 00:59:13.300]   I think prompt engineering becomes a lot easier.
[00:59:13.300 --> 00:59:14.660]   And so having that skill set,
[00:59:14.660 --> 00:59:16.220]   I think is gonna be important.
[00:59:16.220 --> 00:59:18.260]   But I doubt that five years from now,
[00:59:18.260 --> 00:59:20.620]   there will be like a separate job title of prompt engineer.
[00:59:20.620 --> 00:59:21.860]   - Yeah, yeah.
[00:59:21.860 --> 00:59:24.740]   I try to contrast it basically as prompt engineering
[00:59:24.740 --> 00:59:28.820]   is still 2022 and AI engineering is 2023.
[00:59:28.820 --> 00:59:30.940]   But yeah, the central thesis is
[00:59:30.940 --> 00:59:31.940]   you can't just get by with prompts.
[00:59:31.940 --> 00:59:34.500]   You have to write code to manage prompts,
[00:59:34.500 --> 00:59:38.840]   to generate prompts and to generate code
[00:59:38.840 --> 00:59:41.380]   and to for you to evaluate and run that code.
[00:59:41.380 --> 00:59:43.140]   - Yeah, I think I'd agree with all of that.
[00:59:43.140 --> 00:59:45.380]   But to me, that doesn't diminish the importance
[00:59:45.380 --> 00:59:46.220]   of the prompts as an artifact in the system.
[00:59:46.220 --> 00:59:47.660]   - Yes, still important.
[00:59:47.660 --> 00:59:48.940]   - I feel like when I saw Chain of Thought
[00:59:48.940 --> 00:59:51.900]   for the first time, I went from a world in which I was like,
[00:59:51.900 --> 00:59:53.340]   okay, models are not good at reasoning
[00:59:53.340 --> 00:59:55.300]   to models can do some reasoning.
[00:59:55.300 --> 00:59:57.260]   It was a sort of step change in my beliefs
[00:59:57.260 --> 00:59:59.660]   about the capabilities of these models.
[00:59:59.660 --> 01:00:02.260]   And I still think that the LLM Cascades paper
[01:00:02.260 --> 01:00:03.100]   hasn't had the impact that it should have.
[01:00:03.100 --> 01:00:04.820]   - Can you summarize that?
[01:00:04.820 --> 01:00:07.660]   - So this was a paper from Google
[01:00:07.660 --> 01:00:11.500]   and it's just sort of getting you to view LLMs
[01:00:11.500 --> 01:00:13.100]   as a way of doing inference
[01:00:13.100 --> 01:00:14.900]   in a probabilistic programming framework.
[01:00:14.900 --> 01:00:16.100]   So that's a lot of words.
[01:00:16.100 --> 01:00:16.940]   So let me try and sort of unpack that.
[01:00:16.940 --> 01:00:18.940]   - And you have a PhD in this.
[01:00:18.940 --> 01:00:23.420]   - But before AI was all LLMs, there was,
[01:00:23.420 --> 01:00:25.220]   and there still is, a huge branch of research
[01:00:25.220 --> 01:00:26.980]   around probabilistic programs.
[01:00:26.980 --> 01:00:29.660]   So this is just ways of writing code
[01:00:29.660 --> 01:00:32.500]   where probability and random variables
[01:00:32.500 --> 01:00:33.780]   are first class citizen.
[01:00:33.780 --> 01:00:35.140]   So you can have random variables
[01:00:35.140 --> 01:00:36.540]   and then there's lots of different operations
[01:00:36.540 --> 01:00:39.540]   you can do to condition and make predictions about them
[01:00:39.540 --> 01:00:41.180]   and do inferences around them.
[01:00:41.180 --> 01:00:43.380]   And this language modeling Cascades paper
[01:00:43.380 --> 01:00:46.260]   basically said, hey, actually large language models
[01:00:46.260 --> 01:00:49.300]   are a really powerful inference engine
[01:00:49.300 --> 01:00:52.020]   that could be used as a composable piece
[01:00:52.020 --> 01:00:53.300]   inside something that looks like
[01:00:53.300 --> 01:00:55.580]   a probabilistic programming language.
[01:00:55.580 --> 01:00:57.740]   And we were chatting earlier today
[01:00:57.740 --> 01:00:59.580]   about the framework that will emerge
[01:00:59.580 --> 01:01:00.620]   for large language models.
[01:01:00.620 --> 01:01:01.780]   And I know you're working on small
[01:01:01.780 --> 01:01:03.300]   and you've given this a lot of thought
[01:01:03.300 --> 01:01:05.180]   and, you know, LangChain and LamaIndex
[01:01:05.180 --> 01:01:06.660]   and all these different groups,
[01:01:06.660 --> 01:01:09.340]   AutoGPT are trying to circle around
[01:01:09.340 --> 01:01:10.900]   like what's the right set of abstractions?
[01:01:10.900 --> 01:01:12.980]   How might we be able to compose LLMs
[01:01:12.980 --> 01:01:15.380]   in ways to write more complex programs?
[01:01:15.380 --> 01:01:17.020]   And I think that LLM Cascades paper
[01:01:17.020 --> 01:01:19.180]   was one of the first attempts to think about that
[01:01:19.180 --> 01:01:20.900]   in first principles and say,
[01:01:20.900 --> 01:01:22.420]   okay, what are the primitives you might want?
[01:01:22.420 --> 01:01:24.980]   And I think I'm surprised it hasn't been built on more.
[01:01:24.980 --> 01:01:25.980]   - Yeah.
[01:01:25.980 --> 01:01:29.820]   The very, very first AI grant from Nat Friedman
[01:01:29.820 --> 01:01:33.260]   mentioned that they were looking for a UI for Cascades
[01:01:33.260 --> 01:01:34.460]   and no one took them up on it.
[01:01:34.460 --> 01:01:35.300]   - I don't think it needs a UI.
[01:01:35.300 --> 01:01:37.300]   I think it needs a, I think it's a--
[01:01:37.300 --> 01:01:38.140]   - It's a framework.
[01:01:38.140 --> 01:01:38.980]   - It's a framework.
[01:01:38.980 --> 01:01:40.020]   I think you want it in code.
[01:01:40.020 --> 01:01:41.020]   And I would love to work on it
[01:01:41.020 --> 01:01:42.220]   if I had all the time in the world.
[01:01:42.220 --> 01:01:44.340]   It's sort of, you always have to choose your,
[01:01:44.340 --> 01:01:45.580]   you know, you can't do everything at once.
[01:01:45.580 --> 01:01:48.140]   - Well, if someone is working on it, maybe reach out and--
[01:01:48.140 --> 01:01:49.260]   - I would love to chat to people about it
[01:01:49.260 --> 01:01:50.140]   who are working on it. - And give some feedback.
[01:01:50.140 --> 01:01:50.980]   Yeah.
[01:01:50.980 --> 01:01:52.180]   How many of your customers and users
[01:01:52.180 --> 01:01:54.000]   are actually worried about prompt injection
[01:01:54.000 --> 01:01:55.000]   and prompt security?
[01:01:55.000 --> 01:01:57.340]   - Not enough.
[01:01:57.340 --> 01:01:58.180]   - Really?
[01:01:58.180 --> 01:01:59.020]   - So I would say almost zero.
[01:01:59.020 --> 01:01:59.840]   - Yeah.
[01:01:59.840 --> 01:02:01.420]   - And I think that's correct today
[01:02:01.420 --> 01:02:04.340]   because very few of our customers have action-taking LLMs.
[01:02:04.340 --> 01:02:05.180]   - Yeah.
[01:02:05.180 --> 01:02:07.420]   - And I think as long as your models are like read-only,
[01:02:07.420 --> 01:02:08.860]   prompt injection isn't that big a deal.
[01:02:08.860 --> 01:02:10.820]   Like, it's not to me about like leaking your prompts
[01:02:10.820 --> 01:02:12.540]   or something, 'cause the prompts are only really valuable
[01:02:12.540 --> 01:02:14.500]   in the context of your code anyway.
[01:02:14.500 --> 01:02:17.240]   But I do think that once you get to the stage
[01:02:17.240 --> 01:02:19.660]   where you're letting the models have read-write access
[01:02:19.660 --> 01:02:21.260]   to any source of data,
[01:02:21.260 --> 01:02:22.820]   then prompt injection becomes a problem
[01:02:22.820 --> 01:02:25.660]   the same way any other form of code injection is a problem.
[01:02:25.660 --> 01:02:27.460]   But honestly, no one ever asks us about it.
[01:02:27.460 --> 01:02:28.300]   - Right.
[01:02:28.300 --> 01:02:29.120]   - Like, almost never.
[01:02:29.120 --> 01:02:29.960]   - Yeah.
[01:02:29.960 --> 01:02:30.780]   - And I think that's because of the stage
[01:02:30.780 --> 01:02:31.620]   where people are at, right?
[01:02:31.620 --> 01:02:32.700]   Which is that they're still trying
[01:02:32.700 --> 01:02:33.880]   to overcome hallucinations
[01:02:33.880 --> 01:02:35.940]   and they're still trying to put guardrails in place
[01:02:35.940 --> 01:02:37.820]   around the behavior of the models.
[01:02:37.820 --> 01:02:40.340]   And very few people are using agents in production
[01:02:40.340 --> 01:02:41.940]   at meaningfully sized companies.
[01:02:41.940 --> 01:02:42.780]   - Yeah.
[01:02:42.780 --> 01:02:44.940]   - But I think as soon as that becomes the case,
[01:02:44.940 --> 01:02:47.460]   if we do get to a stage where more people
[01:02:47.460 --> 01:02:49.400]   are allowing the models to read from a data source
[01:02:49.400 --> 01:02:50.900]   and write to a data source,
[01:02:50.900 --> 01:02:53.220]   then prompt injection will become something they care about.
[01:02:53.220 --> 01:02:55.100]   - Yeah, and you guys will be well positioned
[01:02:55.100 --> 01:02:56.240]   to offer something.
[01:02:56.240 --> 01:02:57.160]   - Absolutely.
[01:02:57.160 --> 01:03:01.180]   I think sort of being this layer between the raw model
[01:03:01.180 --> 01:03:03.540]   and the end application actually buys us a lot
[01:03:03.540 --> 01:03:04.740]   in terms of what we can help with.
[01:03:04.740 --> 01:03:05.900]   - Yeah, well, you know,
[01:03:05.900 --> 01:03:07.540]   there are a bunch of security-minded people
[01:03:07.540 --> 01:03:10.700]   who are trying to offer that as a standalone thing,
[01:03:10.700 --> 01:03:12.180]   and it's a feature, not a product.
[01:03:12.180 --> 01:03:14.380]   - I think I'd agree with that.
[01:03:14.380 --> 01:03:17.020]   - OpenAI's fine-tuning rollout, which was last month,
[01:03:17.020 --> 01:03:19.180]   how does that affect HumanLoop?
[01:03:19.180 --> 01:03:22.420]   - Yeah, so when we started the first version of HumanLoop,
[01:03:22.420 --> 01:03:24.660]   GPT, chat GPT 3.5 wasn't out yet.
[01:03:24.660 --> 01:03:25.940]   It was all GPT-3,
[01:03:25.940 --> 01:03:28.060]   and we saw a lot of fine-tuning at the time.
[01:03:28.060 --> 01:03:31.820]   And post the release of 3.5 and 4,
[01:03:31.820 --> 01:03:34.340]   by virtue of the fact that it was impossible to fine-tune,
[01:03:34.340 --> 01:03:35.500]   like we could just see it in our analytics,
[01:03:35.500 --> 01:03:37.620]   the amount of fine-tuning just kind of fell off a cliff.
[01:03:37.620 --> 01:03:39.620]   Partly, I think, because the models were better,
[01:03:39.620 --> 01:03:41.660]   but also just partly, like, it wasn't an option.
[01:03:41.660 --> 01:03:44.300]   And so I'm kind of interested to see,
[01:03:44.300 --> 01:03:47.020]   now that 3.5 and 4 fine-tuning are back,
[01:03:47.020 --> 01:03:48.500]   whether that kind of fully recovers--
[01:03:48.500 --> 01:03:49.620]   - 4 isn't back yet, but it's--
[01:03:49.620 --> 01:03:52.900]   - But 3.5 fine-tuning being back.
[01:03:52.900 --> 01:03:55.700]   We've definitely seen a lot in the past,
[01:03:55.700 --> 01:03:58.380]   people generating outputs with GPT-4,
[01:03:58.380 --> 01:04:01.780]   filtering based off evaluation or feedback criteria,
[01:04:01.780 --> 01:04:04.340]   and then fine-tuning smaller, faster models.
[01:04:04.340 --> 01:04:07.740]   And so I think we likely see a lot of fine-tuning
[01:04:07.740 --> 01:04:10.300]   of GPT-3.5 on 4-generated data,
[01:04:10.300 --> 01:04:12.100]   and that's a workflow that we've been,
[01:04:12.100 --> 01:04:14.220]   we natively support within HumanLoop now.
[01:04:14.220 --> 01:04:16.060]   So you can actually kind of do all of those things
[01:04:16.060 --> 01:04:17.620]   without having to leave it.
[01:04:17.620 --> 01:04:18.780]   If you have a bunch of generations,
[01:04:18.780 --> 01:04:20.580]   you can filter them on some criteria,
[01:04:20.580 --> 01:04:22.700]   click fine-tune, run it ahead of evals,
[01:04:22.700 --> 01:04:25.520]   and then decide whether or not to deploy that model.
[01:04:25.520 --> 01:04:27.620]   But time will tell as to whether or not
[01:04:27.620 --> 01:04:30.140]   this is something that goes back up in importance
[01:04:30.140 --> 01:04:31.140]   the way it used to be.
[01:04:31.140 --> 01:04:31.980]   - Yeah.
[01:04:31.980 --> 01:04:33.940]   The follow-up question that occurs to me,
[01:04:33.940 --> 01:04:35.740]   always, we talk about you being that layer
[01:04:35.740 --> 01:04:37.980]   that positions you very well.
[01:04:37.980 --> 01:04:40.280]   A lot of people are fighting to be that layer.
[01:04:40.280 --> 01:04:44.780]   And it occurs to me that as a user,
[01:04:44.780 --> 01:04:47.740]   potentially, of HumanLoop and your competitors,
[01:04:47.740 --> 01:04:51.380]   that I may not want to have to choose or be locked in.
[01:04:51.380 --> 01:04:55.100]   Is there room for an open standard that everyone agrees to,
[01:04:55.100 --> 01:04:56.540]   that we all say, like, okay,
[01:04:56.540 --> 01:04:59.620]   just adopt this one vendor-neutral thing,
[01:04:59.620 --> 01:05:01.180]   and then we all consume from it?
[01:05:01.180 --> 01:05:03.060]   - Maybe.
[01:05:04.180 --> 01:05:05.740]   I think it could happen.
[01:05:05.740 --> 01:05:07.660]   We're not there yet.
[01:05:07.660 --> 01:05:10.580]   I think things are moving too fast for that to be the case,
[01:05:10.580 --> 01:05:12.340]   for people to have clarity on that.
[01:05:12.340 --> 01:05:16.140]   So maybe in the fullness of time, there will be.
[01:05:16.140 --> 01:05:19.100]   My suspicion is that both will happen, right?
[01:05:19.100 --> 01:05:20.720]   That there will be some open standard
[01:05:20.720 --> 01:05:23.100]   that some people like to use.
[01:05:23.100 --> 01:05:25.940]   But once you come to working on
[01:05:25.940 --> 01:05:27.340]   serious production use cases,
[01:05:27.340 --> 01:05:29.180]   you often actually want the peace of mind
[01:05:29.180 --> 01:05:31.160]   of knowing that you're paying a real company
[01:05:31.160 --> 01:05:33.060]   that's gonna be around to support you,
[01:05:33.060 --> 01:05:34.520]   that is focused on this,
[01:05:34.520 --> 01:05:37.380]   that has the knowledge and expertise.
[01:05:37.380 --> 01:05:39.940]   And so, as we've seen in many other spaces,
[01:05:39.940 --> 01:05:41.740]   I suspect that there'll be a bit of both.
[01:05:41.740 --> 01:05:42.820]   - Bit of both, yeah.
[01:05:42.820 --> 01:05:44.580]   So the model I have in mind is Datadog
[01:05:44.580 --> 01:05:47.300]   versus the OpenTelemetry crew.
[01:05:47.300 --> 01:05:48.580]   - Yeah, and Datadog's doing fine,
[01:05:48.580 --> 01:05:51.120]   and the OpenTelemetry crew's doing great as well.
[01:05:51.120 --> 01:05:53.580]   - So last question on the market.
[01:05:53.580 --> 01:05:55.280]   Did GPT-4 get dumber this year?
[01:05:55.280 --> 01:05:57.400]   - I don't think so.
[01:05:57.400 --> 01:06:00.900]   We've seen a lot of conversation about this having happened.
[01:06:00.900 --> 01:06:03.100]   I think GPT-4 changed.
[01:06:03.100 --> 01:06:06.140]   I think that they are regularly updating it,
[01:06:06.140 --> 01:06:07.340]   and you certainly see that
[01:06:07.340 --> 01:06:08.860]   both in sort of people's attempts to,
[01:06:08.860 --> 01:06:09.980]   papers being written about this,
[01:06:09.980 --> 01:06:12.360]   and people are trying to do evaluations over time.
[01:06:12.360 --> 01:06:14.780]   I think that the main takeaway shouldn't be like,
[01:06:14.780 --> 01:06:16.420]   did GPT-4 get dumber, right?
[01:06:16.420 --> 01:06:18.540]   But the interesting question is like, did GPT-4 change?
[01:06:18.540 --> 01:06:20.780]   To which the answer, I think, is definitely yes.
[01:06:20.780 --> 01:06:22.380]   There's no question about that.
[01:06:22.380 --> 01:06:24.420]   And it's something that if you're a developer
[01:06:24.420 --> 01:06:27.060]   building products on top of GPT-4,
[01:06:27.060 --> 01:06:28.540]   it's something that you should think about a lot,
[01:06:28.540 --> 01:06:30.140]   because you're building on a platform
[01:06:30.140 --> 01:06:32.060]   that will evolve and change over time.
[01:06:32.060 --> 01:06:34.700]   And you can pin the base model, but not forever.
[01:06:34.700 --> 01:06:37.460]   And so I think you need to, at the very least,
[01:06:37.460 --> 01:06:39.180]   have really good testing frameworks
[01:06:39.180 --> 01:06:40.380]   to be able to run regression tests
[01:06:40.380 --> 01:06:43.540]   and know, like, have things gotten worse over time, right?
[01:06:43.540 --> 01:06:45.340]   If you can't answer that for yourself,
[01:06:45.340 --> 01:06:46.380]   you're gonna be scratching your head.
[01:06:46.380 --> 01:06:47.540]   Like, did we make the prompts worse?
[01:06:47.540 --> 01:06:49.140]   Did the retrieval system get worse?
[01:06:49.140 --> 01:06:50.260]   Did something else change?
[01:06:50.260 --> 01:06:52.280]   Did the user input distribution change?
[01:06:52.280 --> 01:06:53.560]   Or did the model get worse?
[01:06:53.560 --> 01:06:55.420]   And being able to disentangle those things easily,
[01:06:55.420 --> 01:06:57.620]   I think the importance of that's gonna go up.
[01:06:57.620 --> 01:06:59.140]   But I also think that it should like,
[01:06:59.140 --> 01:07:01.420]   give us pause for thought about kind of the balance
[01:07:01.420 --> 01:07:05.100]   between what gets built on top of third-party providers
[01:07:05.100 --> 01:07:06.340]   and APIs in a closed world,
[01:07:06.340 --> 01:07:09.300]   and what we might wanna do more open source.
[01:07:09.300 --> 01:07:10.620]   And I suspect there'll be a mixture of both,
[01:07:10.620 --> 01:07:11.980]   depending on the use case.
[01:07:11.980 --> 01:07:14.380]   But you are building on shifting sand
[01:07:14.380 --> 01:07:16.220]   whenever you're building on someone else's platform.
[01:07:16.220 --> 01:07:17.860]   - Yeah, yeah, totally.
[01:07:17.860 --> 01:07:19.380]   And then one local specific question
[01:07:19.380 --> 01:07:22.580]   before we go to the takeaway questions.
[01:07:22.580 --> 01:07:24.260]   You went through IC. - Yeah.
[01:07:24.260 --> 01:07:26.080]   - And you are very, very familiar
[01:07:26.080 --> 01:07:27.540]   with the American tech scene.
[01:07:27.540 --> 01:07:30.460]   But also you built your company here in London.
[01:07:30.460 --> 01:07:32.700]   What should, and I'm very US-focused,
[01:07:32.700 --> 01:07:34.860]   most of our audience is very US-centric.
[01:07:34.860 --> 01:07:39.380]   What should people know about the European tech scene?
[01:07:39.380 --> 01:07:41.860]   - Yeah, so I think that London's one of the best places
[01:07:41.860 --> 01:07:45.100]   in the world, and Paris, for AI-focused folks.
[01:07:45.100 --> 01:07:46.340]   - With the hugging phase, I don't know what's--
[01:07:46.340 --> 01:07:47.580]   - We've got a hugging phase in Paris.
[01:07:47.580 --> 01:07:48.640]   Where we're sitting right now,
[01:07:48.640 --> 01:07:50.940]   we're probably less than 200 meters
[01:07:50.940 --> 01:07:52.360]   from the offices of DeepMind.
[01:07:52.360 --> 01:07:54.460]   Facebook AI Research is here as well.
[01:07:54.460 --> 01:07:56.220]   UCL's AI Center is here,
[01:07:56.220 --> 01:07:58.340]   which is where Geoff Hinton was,
[01:07:58.340 --> 01:07:59.180]   and where a lot of great research,
[01:07:59.180 --> 01:08:00.700]   this is where DeepMind spun out of, actually.
[01:08:00.700 --> 01:08:03.480]   So Shane Legge and Demis met at UCL.
[01:08:03.480 --> 01:08:05.660]   So there's an amazing, and there's many more.
[01:08:05.660 --> 01:08:07.180]   I can't list everything that's great,
[01:08:07.180 --> 01:08:10.260]   but there's many great AI institutions in the UK.
[01:08:10.260 --> 01:08:12.240]   What I would say is that I think that Europe
[01:08:12.240 --> 01:08:14.420]   is being amazing on research,
[01:08:14.420 --> 01:08:18.900]   and continues to be a fantastic place for researchers,
[01:08:18.900 --> 01:08:21.020]   but has been less good, in my experience,
[01:08:21.020 --> 01:08:24.420]   on productizing and trying to productize AI.
[01:08:24.420 --> 01:08:26.540]   And so the difference that I feel,
[01:08:26.540 --> 01:08:28.700]   being here versus being in the US,
[01:08:28.700 --> 01:08:31.460]   is just the number of, like if I go to San Francisco,
[01:08:31.460 --> 01:08:34.740]   the density of people who are trying to build useful things
[01:08:34.740 --> 01:08:36.420]   with large language models or with AI,
[01:08:36.420 --> 01:08:38.140]   and butting their head up against it,
[01:08:38.140 --> 01:08:40.380]   and discovering what works and what doesn't work,
[01:08:40.380 --> 01:08:42.940]   and trying great ideas, or trying stupid ideas,
[01:08:42.940 --> 01:08:45.020]   and just learning together,
[01:08:45.020 --> 01:08:46.920]   is much richer than what we have here.
[01:08:46.920 --> 01:08:51.380]   I think the pure research labs, very competitive.
[01:08:51.380 --> 01:08:52.820]   Anthropx just opened an office here.
[01:08:52.820 --> 01:08:55.100]   OpenAI's opening an office here.
[01:08:55.100 --> 01:08:56.260]   When you're hiring for talent,
[01:08:56.260 --> 01:08:58.340]   you'll find as many or better people,
[01:08:58.340 --> 01:09:00.820]   like equal quality people in both places,
[01:09:00.820 --> 01:09:03.460]   but less so once you move towards productization.
[01:09:03.460 --> 01:09:07.080]   And I suspect it's also to do with the investor ecosystem.
[01:09:07.080 --> 01:09:09.420]   So we're sitting in the offices of Local Globe.
[01:09:09.420 --> 01:09:11.180]   Local Globe and Index were our first investors,
[01:09:11.180 --> 01:09:12.660]   and they're both great,
[01:09:12.660 --> 01:09:14.660]   but the number of investors that you have
[01:09:14.660 --> 01:09:17.420]   of that quality in Europe is not the same as the US,
[01:09:17.420 --> 01:09:18.740]   and the type of people you interact with
[01:09:18.740 --> 01:09:19.700]   are very different.
[01:09:19.700 --> 01:09:21.140]   When I speak to VCs in the US,
[01:09:21.140 --> 01:09:22.940]   there's way more former founders,
[01:09:22.940 --> 01:09:26.040]   there's way more people who have done DevTools before,
[01:09:26.040 --> 01:09:27.980]   and there's way more support from the founders
[01:09:27.980 --> 01:09:30.700]   towards the ecosystem than there is in Europe.
[01:09:30.700 --> 01:09:33.940]   People are trying, but the culture is not quite the same.
[01:09:33.940 --> 01:09:35.980]   And that's why we're moving to SF, right?
[01:09:35.980 --> 01:09:38.020]   We want to be, every time I've been to SF,
[01:09:38.020 --> 01:09:39.620]   good things have happened to people.
[01:09:39.620 --> 01:09:42.000]   Whether it's like bumping into you,
[01:09:42.000 --> 01:09:44.340]   or we get an introduction to an interesting investor,
[01:09:44.340 --> 01:09:47.060]   or a customer, or we just speak to someone
[01:09:47.060 --> 01:09:49.220]   who's been trying really hard to build something.
[01:09:49.220 --> 01:09:52.420]   And we share an office here in London with Bloop,
[01:09:52.420 --> 01:09:55.500]   that does, Bloop AI does code search with LLMs,
[01:09:55.500 --> 01:09:57.300]   and we've tried our very best
[01:09:57.300 --> 01:09:59.180]   to kind of aggregate a few other companies to us,
[01:09:59.180 --> 01:10:02.220]   and we're doing AI tinkerers tomorrow.
[01:10:02.220 --> 01:10:03.660]   So there is some of it here,
[01:10:03.660 --> 01:10:05.820]   but you have to work so much harder.
[01:10:05.820 --> 01:10:09.180]   Versus in SF, you can't move for hitting some AI thing.
[01:10:09.180 --> 01:10:11.060]   - We had a Thursday recently
[01:10:11.060 --> 01:10:12.980]   with 10 AI meetups in one night.
[01:10:12.980 --> 01:10:14.300]   - Yeah, it's almost too much.
[01:10:14.300 --> 01:10:15.140]   - It is too much.
[01:10:15.140 --> 01:10:15.980]   (laughing)
[01:10:15.980 --> 01:10:17.980]   I'll go there and say it is too much.
[01:10:17.980 --> 01:10:19.300]   - Yeah, yeah, yeah.
[01:10:19.300 --> 01:10:20.260]   - You need some time to build things too.
[01:10:20.260 --> 01:10:21.420]   - And there is, I would say,
[01:10:21.420 --> 01:10:23.180]   actually in the SF builder scene,
[01:10:23.180 --> 01:10:24.940]   privilege that comes out of
[01:10:24.940 --> 01:10:27.260]   just having so much opportunity thrown at you.
[01:10:27.260 --> 01:10:31.540]   And that we have this arm's length, this taste for VC,
[01:10:31.540 --> 01:10:33.100]   and I'm like, no, they are partners
[01:10:33.100 --> 01:10:33.940]   in building your business.
[01:10:33.940 --> 01:10:34.900]   - Oh, absolutely.
[01:10:34.900 --> 01:10:37.180]   - Yeah, so I think it's an interesting contrast,
[01:10:37.180 --> 01:10:38.940]   but as a person, I'm not American,
[01:10:38.940 --> 01:10:41.300]   I lived most of my adult life in America,
[01:10:41.300 --> 01:10:46.300]   but I feel for non-US policy makers,
[01:10:46.420 --> 01:10:50.100]   and VCs, and people who care about their city,
[01:10:50.100 --> 01:10:53.180]   who are like, okay, we're not SF, what do we do?
[01:10:53.180 --> 01:10:54.580]   - I honestly think that it's,
[01:10:54.580 --> 01:10:55.820]   we think a lot about network effects
[01:10:55.820 --> 01:10:57.580]   and defensibility in startups.
[01:10:57.580 --> 01:10:59.860]   I think it's like the mother of all network effects.
[01:10:59.860 --> 01:11:02.020]   The reason I'm going is not because I love the city.
[01:11:02.020 --> 01:11:04.220]   I mean, SF's fine as a city, I like it,
[01:11:04.220 --> 01:11:06.300]   but I'm going because everyone else is going,
[01:11:06.300 --> 01:11:09.340]   and everyone else is going because we're going.
[01:11:09.340 --> 01:11:11.300]   Once you've attracted a certain talent density,
[01:11:11.300 --> 01:11:14.100]   I think it's really hard to compete with that.
[01:11:14.100 --> 01:11:15.140]   - Oh boy, okay.
[01:11:15.140 --> 01:11:15.980]   (laughing)
[01:11:15.980 --> 01:11:18.700]   - It is true, it's the honest truth.
[01:11:18.700 --> 01:11:21.900]   I do want to work on a path for non-tech hub cities,
[01:11:21.900 --> 01:11:23.780]   because that's where I'm from.
[01:11:23.780 --> 01:11:25.780]   - Yeah, and me too as well.
[01:11:25.780 --> 01:11:29.060]   But I also think there's something to be said
[01:11:29.060 --> 01:11:32.060]   for the most driven, most ambitious people
[01:11:32.060 --> 01:11:35.060]   finding a way to get to where the center for their thing is.
[01:11:35.060 --> 01:11:37.460]   And right now, today, for AI-focused products,
[01:11:37.460 --> 01:11:38.840]   I think it's San Francisco,
[01:11:38.840 --> 01:11:42.180]   but for different things, the center is different places.
[01:11:42.180 --> 01:11:44.260]   Hollywood is the place to go if you're an actor or whatever,
[01:11:44.260 --> 01:11:46.700]   and there are different hubs for different areas.
[01:11:46.700 --> 01:11:47.980]   - It's a Paul Graham thing.
[01:11:47.980 --> 01:11:50.700]   Different cities breathe different ambitions into you,
[01:11:50.700 --> 01:11:53.060]   and in San Francisco, apparently it's power.
[01:11:53.060 --> 01:11:54.460]   (laughing)
[01:11:54.460 --> 01:11:56.020]   It's not actually tech, it's power.
[01:11:56.020 --> 01:11:56.860]   - Okay, interesting.
[01:11:56.860 --> 01:11:58.500]   - And tech is a means to power.
[01:11:58.500 --> 01:11:59.340]   - Interesting.
[01:11:59.340 --> 01:12:01.460]   There's a lesson in that for those of us
[01:12:01.460 --> 01:12:02.780]   who think about AGI safety.
[01:12:02.780 --> 01:12:06.900]   - And also, not anywhere in San Francisco,
[01:12:06.900 --> 01:12:09.300]   specific two square miles in San Francisco called the arena.
[01:12:09.300 --> 01:12:10.140]   (laughing)
[01:12:10.140 --> 01:12:12.140]   You have to get in the arena and build.
[01:12:12.140 --> 01:12:15.060]   - Okay, so broader takeaway questions.
[01:12:15.060 --> 01:12:16.500]   We always ask three of all our guests.
[01:12:16.500 --> 01:12:18.180]   Acceleration, what has already happened in AI
[01:12:18.180 --> 01:12:20.420]   that you thought would take much longer?
[01:12:20.420 --> 01:12:23.380]   - So this has been, since I started my PhD,
[01:12:23.380 --> 01:12:24.660]   every year things have happened
[01:12:24.660 --> 01:12:25.740]   that I thought would take much longer.
[01:12:25.740 --> 01:12:27.740]   So when I started my PhD, it was at a time
[01:12:27.740 --> 01:12:30.160]   when deep learning had just sort of started working,
[01:12:30.160 --> 01:12:32.380]   and transfer learning, even for vision,
[01:12:32.380 --> 01:12:33.940]   hadn't been figured out yet.
[01:12:33.940 --> 01:12:35.740]   And people were talking about,
[01:12:35.740 --> 01:12:37.220]   how long before we can train models
[01:12:37.220 --> 01:12:39.540]   that don't need millions of annotated data examples?
[01:12:39.540 --> 01:12:41.580]   How long, so AlphaGo was happening
[01:12:41.580 --> 01:12:44.180]   just at that point in time, the first version.
[01:12:44.180 --> 01:12:45.660]   I have made predictions and been wrong
[01:12:45.660 --> 01:12:46.540]   again and again and again.
[01:12:46.540 --> 01:12:48.340]   I've just been consistently too pessimistic.
[01:12:48.340 --> 01:12:50.500]   And I think I'm quite an optimistic person.
[01:12:50.500 --> 01:12:54.160]   When would, like Dota surprised me when it happened.
[01:12:54.160 --> 01:12:57.680]   The first transfer learning working in vision
[01:12:57.680 --> 01:12:58.660]   surprised me when it happened.
[01:12:58.660 --> 01:13:00.860]   The continued success of scale and deep learning.
[01:13:00.860 --> 01:13:03.660]   And then finally, although I believed
[01:13:03.660 --> 01:13:04.920]   that LLMs were gonna be enormous,
[01:13:04.920 --> 01:13:07.320]   and I thought GPT-3 was gonna be the future,
[01:13:07.320 --> 01:13:10.720]   just how good GPT-4 and ChatGPT turned out to be
[01:13:10.720 --> 01:13:11.840]   did surprise me.
[01:13:11.840 --> 01:13:13.460]   The first time, I actually saw Claude
[01:13:13.460 --> 01:13:15.800]   before I saw ChatGPT.
[01:13:15.800 --> 01:13:17.740]   But the first time I saw Claude,
[01:13:17.740 --> 01:13:20.200]   and I kept pushing the limits of it
[01:13:20.200 --> 01:13:22.460]   with tasks that I knew were kind of at the frontier
[01:13:22.460 --> 01:13:24.080]   of what was currently possible,
[01:13:24.080 --> 01:13:27.080]   and just saw it blasting through these one after another.
[01:13:27.080 --> 01:13:29.080]   That was a mind-blowing moment for me.
[01:13:29.080 --> 01:13:31.160]   And I think it was for a lot of the rest of us.
[01:13:31.160 --> 01:13:32.520]   I think we're gonna have a lot more of those.
[01:13:32.520 --> 01:13:33.960]   I think that's gonna keep happening.
[01:13:33.960 --> 01:13:36.960]   - Yeah, yeah, we are accelerating as we speak.
[01:13:36.960 --> 01:13:38.320]   Exploration, what do you think
[01:13:38.320 --> 01:13:41.120]   is the most interesting unsolved question in AI?
[01:13:41.120 --> 01:13:43.120]   - I think there's actually some obvious
[01:13:43.120 --> 01:13:44.960]   kind of elephant in the room unsolved problems
[01:13:44.960 --> 01:13:46.480]   that for some reason don't seem to get
[01:13:46.480 --> 01:13:48.840]   the amount of airtime that they kind of obviously should.
[01:13:48.840 --> 01:13:50.800]   So continual learning to me is one of these.
[01:13:50.800 --> 01:13:51.640]   - Oh God, yeah.
[01:13:51.640 --> 01:13:53.960]   - Like we all walk around as if it's just completely normal
[01:13:53.960 --> 01:13:55.880]   that these models never learn anything new.
[01:13:55.880 --> 01:13:57.560]   - Yeah, 2021 is when history ended.
[01:13:57.560 --> 01:13:59.880]   - You just think, yeah, 2021 is when history ended.
[01:13:59.880 --> 01:14:02.360]   And you do retrieval augmentation with a vector database,
[01:14:02.360 --> 01:14:03.440]   and you're done, right?
[01:14:03.440 --> 01:14:05.880]   Why would the system keep learning after training?
[01:14:05.880 --> 01:14:08.760]   And I think everyone knows that this is a problem,
[01:14:08.760 --> 01:14:11.680]   but somehow it doesn't seem to me to get the amount of,
[01:14:11.680 --> 01:14:14.880]   I think this field in research is called continual learning
[01:14:14.880 --> 01:14:17.160]   or lifelong learning.
[01:14:17.160 --> 01:14:19.840]   And it doesn't seem to get the airtime that it used to.
[01:14:19.840 --> 01:14:22.440]   It seems to me like an obviously enormous problem.
[01:14:22.440 --> 01:14:25.640]   The other one that I think will happen naturally,
[01:14:25.640 --> 01:14:27.360]   but just hasn't happened yet,
[01:14:27.360 --> 01:14:29.440]   is just like more multimodality, right?
[01:14:29.440 --> 01:14:31.040]   Like it's kind of obvious that these models
[01:14:31.040 --> 01:14:35.120]   should be plugged in to vision, audio, speech, et cetera,
[01:14:35.120 --> 01:14:36.480]   and have shared representations,
[01:14:36.480 --> 01:14:38.240]   'cause there's so much to be gained from that.
[01:14:38.240 --> 01:14:40.760]   And I think it's just like gonna happen with time,
[01:14:40.760 --> 01:14:42.240]   but hasn't happened yet.
[01:14:42.240 --> 01:14:46.960]   - Yeah, well, I think the cost is just token space, I guess.
[01:14:46.960 --> 01:14:49.000]   I don't know how much more you need
[01:14:49.000 --> 01:14:51.360]   to add every single modality.
[01:14:51.360 --> 01:14:52.200]   Although I think Facebook released like six modalities.
[01:14:52.200 --> 01:14:54.000]   - We have some examples of this, right?
[01:14:54.000 --> 01:14:57.520]   So like Gatto from DeepMind was a transformer model
[01:14:57.520 --> 01:14:58.520]   that they trained across.
[01:14:58.520 --> 01:15:00.080]   They just did policy distillation.
[01:15:00.080 --> 01:15:02.360]   So they trained a whole bunch of different RL agents,
[01:15:02.360 --> 01:15:03.520]   and they took the outputs of that,
[01:15:03.520 --> 01:15:05.840]   which is like observation, action, reward, triples,
[01:15:05.840 --> 01:15:08.720]   and trained a single transformer model on all of that.
[01:15:08.720 --> 01:15:11.120]   And then that one model could do any of those tasks.
[01:15:11.120 --> 01:15:12.640]   Actually, okay, whilst we're in exploration mode,
[01:15:12.640 --> 01:15:13.960]   there's a paper from DeepMind,
[01:15:13.960 --> 01:15:16.600]   very same time, came out at the same time as Gatto,
[01:15:16.600 --> 01:15:19.280]   that I think is massively underrated.
[01:15:19.280 --> 01:15:21.640]   And I don't understand why it didn't get more attention,
[01:15:21.640 --> 01:15:24.120]   which it was at the same NeurIPS conference,
[01:15:24.120 --> 01:15:25.240]   and I forget the exact title,
[01:15:25.240 --> 01:15:26.080]   but I think it's called like
[01:15:26.080 --> 01:15:29.120]   in-context reinforcement learning, or something like that.
[01:15:29.120 --> 01:15:31.200]   And they do something really similar to Gatto.
[01:15:31.200 --> 01:15:34.120]   They take an RL agent, they train it,
[01:15:34.120 --> 01:15:37.320]   and then they distill that into a transformer model.
[01:15:37.320 --> 01:15:38.700]   But what they do that's different
[01:15:38.700 --> 01:15:40.760]   is they don't take the trained RL agent.
[01:15:40.760 --> 01:15:43.720]   Instead, they take an untrained RL agent,
[01:15:43.720 --> 01:15:45.960]   and they record the full trajectory of its learning.
[01:15:45.960 --> 01:15:48.360]   So early on in the data, the model's kind of crappy,
[01:15:48.360 --> 01:15:49.640]   and by the end of the data,
[01:15:49.640 --> 01:15:51.600]   the model's been good at this task.
[01:15:51.600 --> 01:15:53.400]   And then they train a transformer model
[01:15:53.400 --> 01:15:55.920]   to predict that sequence.
[01:15:55.920 --> 01:15:58.240]   And in order to be good at predicting that sequence,
[01:15:58.240 --> 01:16:00.720]   you have to predict that the sub-agent,
[01:16:00.720 --> 01:16:02.280]   like the RL agent that generated the data,
[01:16:02.280 --> 01:16:04.480]   gets better at the task over time.
[01:16:04.480 --> 01:16:06.760]   And the only way that I can see to do that,
[01:16:06.760 --> 01:16:08.960]   and in fact, this seems to be what the model's doing,
[01:16:08.960 --> 01:16:11.920]   is that you have to simulate a learning algorithm.
[01:16:11.920 --> 01:16:13.480]   The transformer has to simulate
[01:16:13.480 --> 01:16:15.280]   in-context reinforcement learning.
[01:16:15.280 --> 01:16:17.200]   And so they take all of these tasks,
[01:16:17.200 --> 01:16:19.220]   they train on the learning trajectories,
[01:16:19.220 --> 01:16:21.000]   and then they take a completely new task
[01:16:21.000 --> 01:16:23.320]   that that transformer model has never seen before,
[01:16:23.320 --> 01:16:25.280]   and it learns to do that task.
[01:16:25.280 --> 01:16:27.720]   And so it's learning from reward signals
[01:16:27.720 --> 01:16:29.440]   in context to achieve a new task.
[01:16:29.440 --> 01:16:30.720]   And to me, that's huge.
[01:16:30.720 --> 01:16:33.360]   It's a demonstration of inner optimization
[01:16:33.360 --> 01:16:35.120]   within a transformer model,
[01:16:35.120 --> 01:16:37.600]   and it's also a demonstration of in-context,
[01:16:37.600 --> 01:16:39.720]   continuous learning that's limited only
[01:16:39.720 --> 01:16:41.240]   by the length of the context window.
[01:16:41.240 --> 01:16:42.520]   If the context window was really long,
[01:16:42.520 --> 01:16:44.240]   you could make this work practically.
[01:16:44.240 --> 01:16:46.800]   I don't really know why that wasn't a bigger deal.
[01:16:46.800 --> 01:16:48.780]   - I don't know either.
[01:16:48.780 --> 01:16:50.320]   This sounds fantastic.
[01:16:50.320 --> 01:16:52.240]   - Yeah, and Gato, I think the reason
[01:16:52.240 --> 01:16:53.080]   maybe it wasn't a bigger deal
[01:16:53.080 --> 01:16:54.560]   is it came out exactly the same time as Gato,
[01:16:54.560 --> 01:16:56.980]   and I think Gato just took all the attention.
[01:16:56.980 --> 01:16:59.740]   - So we just got done talking a lot about focus,
[01:16:59.740 --> 01:17:02.300]   but given that you see potential in this,
[01:17:02.300 --> 01:17:05.460]   and this would be huge for literally training anything,
[01:17:05.460 --> 01:17:08.280]   would you be interested in exploring it at some point?
[01:17:08.280 --> 01:17:09.780]   - As in trying to train it myself?
[01:17:09.780 --> 01:17:11.140]   - Put this in production,
[01:17:11.140 --> 01:17:12.740]   some form of continuous learning.
[01:17:12.740 --> 01:17:15.620]   Obviously, that's on your radar, continuous learning.
[01:17:15.620 --> 01:17:17.620]   - I would love to, but I think you have to decide
[01:17:17.620 --> 01:17:19.340]   what kind of company you want to be,
[01:17:19.340 --> 01:17:22.660]   and this is something for OpenAI or Anthropic to focus on.
[01:17:22.660 --> 01:17:24.140]   I feel like you have to be thinking
[01:17:24.140 --> 01:17:26.580]   about the fundamentals of,
[01:17:26.580 --> 01:17:28.740]   this is the kind of research I used to do as a PhD student.
[01:17:28.740 --> 01:17:30.740]   - So I'll put it this way, right?
[01:17:30.740 --> 01:17:33.620]   You have the research background to do this,
[01:17:33.620 --> 01:17:35.240]   and you're choosing not to,
[01:17:35.240 --> 01:17:36.460]   and you're building a company
[01:17:36.460 --> 01:17:40.160]   that doesn't use your research, specifically that part.
[01:17:40.160 --> 01:17:43.620]   I mean, you know.
[01:17:43.620 --> 01:17:47.540]   - Reasonable question, but I think that I'm excited
[01:17:47.540 --> 01:17:52.100]   about getting things useful into people's hands very quickly.
[01:17:52.100 --> 01:17:54.300]   Like I like seeing, we talked about this earlier, right?
[01:17:54.300 --> 01:17:56.100]   We've moved from the research phase
[01:17:56.100 --> 01:17:57.740]   to the engineering phase of AI.
[01:17:57.740 --> 01:18:00.580]   It's the first time after having been in this field
[01:18:00.580 --> 01:18:02.060]   for maybe seven years,
[01:18:02.060 --> 01:18:07.060]   where stuff goes beyond just kind of a graph, right?
[01:18:07.060 --> 01:18:09.300]   The output of my work before would always be like,
[01:18:09.300 --> 01:18:11.780]   oh look, there's a graph, and the number's better now,
[01:18:11.780 --> 01:18:13.740]   versus we actually get to see,
[01:18:13.740 --> 01:18:16.140]   we have a customer, between Duolingo
[01:18:16.140 --> 01:18:17.460]   and two or three of our other customers,
[01:18:17.460 --> 01:18:19.060]   we've got three or four customers working on
[01:18:19.060 --> 01:18:21.180]   better versions of teaching students, right?
[01:18:21.180 --> 01:18:24.380]   Tutors or language learning or whatever it might be,
[01:18:24.380 --> 01:18:26.300]   and to be able to make that incrementally better
[01:18:26.300 --> 01:18:28.900]   and accelerate the time it takes to get there,
[01:18:28.900 --> 01:18:30.820]   it just feels to be so much closer to it,
[01:18:30.820 --> 01:18:32.700]   to be on the engineering space right now,
[01:18:32.700 --> 01:18:34.700]   whereas I think there's an alternative universe
[01:18:34.700 --> 01:18:36.580]   in which I stayed in research,
[01:18:36.580 --> 01:18:39.260]   and I went to an open AI, or you know,
[01:18:39.260 --> 01:18:41.340]   almost everyone from my research, PhD research group,
[01:18:41.340 --> 01:18:43.300]   apart from Peter, now works at DeepMind.
[01:18:43.300 --> 01:18:46.900]   And I think I would have enjoyed that as well,
[01:18:46.900 --> 01:18:48.300]   but I really wanted to start a company
[01:18:48.300 --> 01:18:50.940]   that built something useful and in production,
[01:18:50.940 --> 01:18:52.240]   and I don't even think those companies
[01:18:52.240 --> 01:18:53.380]   do that much right now, right?
[01:18:53.380 --> 01:18:55.340]   Like, it's only recently that open AI
[01:18:55.340 --> 01:18:56.980]   has sort of become a product company.
[01:18:56.980 --> 01:18:58.620]   They're more of a research company,
[01:18:58.620 --> 01:18:59.460]   they're building AGI,
[01:18:59.460 --> 01:19:01.020]   and I think that's true of the others,
[01:19:01.020 --> 01:19:02.820]   and I think that's amazing and fascinating,
[01:19:02.820 --> 01:19:05.900]   and if I had multiple lives, I would love to do that too,
[01:19:05.900 --> 01:19:07.900]   but at least right now, I wanna be building products
[01:19:07.900 --> 01:19:09.260]   and putting them in people's hands,
[01:19:09.260 --> 01:19:10.900]   and it just feels a little bit far removed.
[01:19:10.900 --> 01:19:12.220]   - Yeah, yeah, makes sense.
[01:19:12.220 --> 01:19:13.860]   And I think the world's better
[01:19:13.860 --> 01:19:15.660]   because you're actually coming at it
[01:19:15.660 --> 01:19:18.780]   with a full knowledge of what came before.
[01:19:18.780 --> 01:19:21.300]   - Yeah, I do think it's a huge advantage.
[01:19:21.300 --> 01:19:23.540]   I do think having a good conceptual,
[01:19:23.540 --> 01:19:25.340]   like, there's been a lot of people
[01:19:25.340 --> 01:19:27.940]   that have pivoted into, as you said, LLM ops earlier,
[01:19:27.940 --> 01:19:31.260]   and I do think that actually knowing how it works,
[01:19:31.260 --> 01:19:33.100]   having a sense of what's gonna come next,
[01:19:33.100 --> 01:19:35.460]   and being able to project forwards and build for it,
[01:19:35.460 --> 01:19:36.780]   is difficult to do if you don't have
[01:19:36.780 --> 01:19:38.900]   a good conceptual understanding of the machine learning.
[01:19:38.900 --> 01:19:40.580]   - Yeah, yeah, yeah, agreed.
[01:19:40.580 --> 01:19:42.900]   Okay, well, I feel like this is a leading question,
[01:19:42.900 --> 01:19:46.220]   but what's one message you want everyone to take away today?
[01:19:46.220 --> 01:19:48.260]   - Oh, wow, that's a great question.
[01:19:48.260 --> 01:19:50.060]   Really, if you're building,
[01:19:50.060 --> 01:19:51.980]   if you're building a serious LLM application
[01:19:51.980 --> 01:19:54.300]   and you're trying to do, find the right prompts,
[01:19:54.300 --> 01:19:56.140]   optimize them, evaluate your models,
[01:19:56.140 --> 01:19:58.060]   then I really would encourage you to try out HumanLoop.
[01:19:58.060 --> 01:20:00.660]   Like, that's the use case that we really solve well for,
[01:20:00.660 --> 01:20:02.500]   especially if you're kind of having to collaborate
[01:20:02.500 --> 01:20:03.900]   with non-technical people,
[01:20:03.900 --> 01:20:06.100]   then HumanLoop will probably solve a lot of pain for you.
[01:20:06.100 --> 01:20:06.980]   - Yeah, excellent.
[01:20:06.980 --> 01:20:08.580]   Well, thanks so much for doing this.
[01:20:08.580 --> 01:20:10.780]   I had a real joy getting to know you
[01:20:10.780 --> 01:20:13.380]   and debugging real life issues with you,
[01:20:13.380 --> 01:20:15.500]   but that's the fun of latent space, so thank you so much.
[01:20:15.500 --> 01:20:16.340]   - No, thanks for having me.
[01:20:16.340 --> 01:20:17.380]   It's been an absolute pleasure to get
[01:20:17.380 --> 01:20:18.780]   to spend some time with you, Sean.
[01:20:18.780 --> 01:20:22.220]   (upbeat music)
[01:20:22.220 --> 01:20:24.260]   - In this episode of the Latent Space Podcast,
[01:20:24.260 --> 01:20:26.860]   we delved into the world of LLM ops
[01:20:26.860 --> 01:20:28.460]   and had a wide ranging conversation
[01:20:28.460 --> 01:20:31.500]   with Dr. Raza Habib, co-founder of HumanLoop.
[01:20:31.500 --> 01:20:33.860]   We covered what is HumanLoop,
[01:20:33.860 --> 01:20:36.220]   the three stages of prompt evals,
[01:20:36.220 --> 01:20:38.300]   the three types of human feedback,
[01:20:38.300 --> 01:20:40.780]   HumanLoop's new free tier and pricing,
[01:20:40.780 --> 01:20:44.060]   the competitive landscape and graduation risk of HumanLoop,
[01:20:44.060 --> 01:20:48.580]   prompt ops versus MLops, prompt engineer versus AI engineer.
[01:20:48.580 --> 01:20:50.740]   Did GPT-4 get dumber?
[01:20:50.740 --> 01:20:53.140]   Europe's AI scene versus San Francisco.
[01:20:53.140 --> 01:20:55.820]   And don't sleep on Raza's in-depth explanations
[01:20:55.820 --> 01:20:59.980]   of LLM cascades and DeepMind's work on continuous learning.
[01:20:59.980 --> 01:21:01.900]   If you are interested in HumanLoop,
[01:21:01.900 --> 01:21:04.580]   definitely check out their hiring page and new pricing
[01:21:04.580 --> 01:21:07.660]   and vote for them on the state of AI engineering survey.
[01:21:07.660 --> 01:21:10.660]   Thank you for tuning into the Latent Space Podcast.
[01:21:10.660 --> 01:21:12.140]   Don't forget to like, subscribe,
[01:21:12.140 --> 01:21:14.340]   and tweet your takes at Latent Space Pod.
[01:21:14.340 --> 01:21:15.260]   Now go build.
[01:21:15.260 --> 01:21:17.840]   (upbeat music)

